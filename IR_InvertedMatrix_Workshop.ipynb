{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890288b1",
   "metadata": {},
   "source": [
    "# ðŸ› ï¸ Active Learning Workshop: Implementing an Inverted Matrix (Jupyter + GitHub Edition)\n",
    "## ðŸ” Workshop Theme\n",
    "*Readable, correct, and collaboratively reviewed codeâ€”just like in the real world.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e78d18",
   "metadata": {},
   "source": [
    "\n",
    "Welcome to the 90-minute workshop! In this hands-on session, your team will build an **Inverted Index** pipeline, the foundation of many intelligent systems that need fast and relevant access to text data â€” such as AI agents.\n",
    "\n",
    "### ðŸ‘¥ Team Guidelines\n",
    "- Work in teams of 3.\n",
    "- Submit one completed Jupyter Notebook per team.\n",
    "- The final notebook must contain **Markdown explanations** and **Python code**.\n",
    "- Push your notebook to GitHub and share the `.git` link before class ends.\n",
    "\n",
    "---\n",
    "## ðŸ”§ Workshop Tasks Overview\n",
    "\n",
    "1. **Document Collection**\n",
    "2. **Tokenizer Implementation**\n",
    "3. **Normalization Pipeline (Stemming, Stop Words, etc.)**\n",
    "4. **Build and Query the Inverted Index**\n",
    "\n",
    "> Each step includes a sample **talking point**. Your team must add your own custom **Markdown + code cells** with a **second talking point**, and test your Inverted Index with **2 phrase queries**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a922333",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ðŸ§  Learning Objectives\n",
    "- Implement an **Inverted Matrix** using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into peer notebooks.\n",
    "- Practice **collaborative debugging**, professional peer feedback, and improve code quality.\n",
    "\n",
    "## ðŸ§© Workshop Structure (90 Minutes)\n",
    "1. **Instructor Use Case Introduction** *(15 min)* â€“ Set up teams of 3 people. Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Team Jupyter Notebook Development** *(45 min)* â€“ Manual IR and Inverted Matrix coding + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** *(15 min)* â€“ Teams commit and push initial notebooks. **Make sure to include your names so it is easy to identify the team that developed the Min-Max code**.\n",
    "4. **Instructor Review** - The instructor will go around, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "5. **Email Delivery** *(15 min)* â€“ Each team send the instructor an email **with the *.git link** to the GitHub repo **(one email/team)**. Subject on the email is: PROG8245 - Inverted Matrix  Workshop, Team #_____.\n",
    "\n",
    "\n",
    "## ðŸ’» Submission Checklist\n",
    "- âœ… `IR_InvertedMatrix_Workshop.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline, and Inverted Index.\n",
    "  - Markdown explanations for each major step\n",
    "  - **Labeled talking point(s)** and 2 phrase query tests\n",
    "- âœ… `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- âœ… GitHub Repo:\n",
    "  - Public repo named `IR-invertedmatrix-workshop`\n",
    "  - This is a group effort, so **choose one member of the team** to publish the repo\n",
    "  - At least **one commit containing one meaningful talking point**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e160c9d",
   "metadata": {},
   "source": [
    "## ðŸ“„ Step 1: Document Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc964464",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ—£ Instructor Talking Point:\n",
    "> We begin by gathering a text corpus. To build a robust index, your vocabulary should include **over 2000 unique words**. You can use scraped articles, academic papers, or open datasets.\n",
    "\n",
    "### ðŸ”§ Your Task:\n",
    "- Collect at least 20+ text documents.\n",
    "- Ensure the vocabulary exceeds 2000 unique words.\n",
    "- Load the documents into a list for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23ee0c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 documents.\n"
     ]
    }
   ],
   "source": [
    "# Example: Load text files from a folder\n",
    "import os\n",
    "\n",
    "input_dir = 'sample_docs/'\n",
    "\n",
    "def load_documents(folder_path):\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                documents.append(file.read())\n",
    "    return documents\n",
    "\n",
    "# Replace 'sample_docs/' with your actual folder\n",
    "documents = load_documents(input_dir)\n",
    "print(f\"Loaded {len(documents)} documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b342945a",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ Step 2: Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2803fb52",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ—£ Instructor Talking Point:\n",
    "> The tokenizer breaks raw text into a stream of words (tokens). This is the foundation for every later step in IR and NLP.\n",
    "\n",
    "### ðŸ”§ Your Task:\n",
    "- Implement a basic tokenizer that splits text into lowercase words.\n",
    "- Handle punctuation removal and basic non-alphanumeric filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7c3654e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TOKENIZING FIRST 20 FILES FROM 'sample_docs/' ---\n",
      "\n",
      "{'likely', 'aid', 'more', 'groups', 'bear', 'matt', 'speed', 'memory', 'case', 'break', 'price', 'consecutive', 'clever', 'just', 'untrustworthy', 'accused', '00', 'argumentation', 'firearms', 'but', 'far', 'must', 'russian', 'ordo', 'through', 'gladly', 'bus', 'defense', 'worked', 'raised', 'x', 'surface', 'explanations', 'aquire', 'turanist', 'simplistic', 'laws', 'components', '10_', 'heinrich', 'session', 'schooled', 'protects', 'weapons', 'go', 'jasmine', '1905', 'express', 'few', 'hearded', 'attempts', 'teletype', 'major', 'reasonably', 'aura', 'none', 'publish', 'growth', 'uerdugo', 'praise', 'drive', 'deleted', 'understanding', 'practical', 'such', '120kvolt', '8', 'confused', 'stretching', 'background', 'conditions', 'test', 'bmf', 'augsburg', 'kurds', 'ability', 'days', 'task', 'kind', 'work', 'liberty', 'divinity', 'introduction', 'weiser', 'country', 'tv', 'principle', 'permit', 'games', 'fox', 'adriatic', 'que', 'settled', 'book', 'manyk', 'wings', 'connect', 'explicitly', 'wants', 'wealthy', 'two', 'karl', 'health', 'possess', 'critical', 'explain', 'surprises', '1993', '20', 'displays', 'fascist', 'nothing', 'choice', 'represent', 'doctrines', 'doing', 'serious', 'assess', 'worse', 'appointed', 'com', 'seems', '97th', 'presented', 'occur', 'equinox', 'preserve', 'masonry', 'reasons', 'thread', 'christmas', 'tsn', 'ap', 'paper', 'or', 'thus', 'knew', 'other', 'moment', 'parts', 'letter', 'what', 'regular', 'assistant', 'puzzled', 'ultra', 'writing', 'stands', 'tape', 'context', 'os', 'extend', 'fuhr', 'often', 'guidelines', 'hope', 'own', 'longstanding', 'rev', 'resigned', 'artists', 'card', 'deeply', 'takes', 'oriflamme', 'bell', 'giving', 'eye', 'closely', 'area', 'keep', 'busy', 'organization', 'give', 'doubts', 'job', 'infiltrating', 'does', 'countersteer', 'once', 'suggestions', 'came', 'bowman', 'accepted', 'further', '7', 'computers', 'mighty', 'together', 'singer', 'change', 'brake', 'best', 'wish', 'army', 'self', 'spectacular', 'starting', 'beta', 'related', 'for', 'less', 'pens', 'while', 'gic', 'circle', 'leader', 'side', 'specific', 'ed', 'apprehend', 'various', 'expected', 'yeah', 'pass', 'people', 'permitted', 'examiner', 'austrian', 'believes', 'steiner', 'radius', 'phrase', '5', 'her', 'rpm', 'azeris', '6', 'means', 'complex', 'mason', 'rapings', 'audience', 'concept', 'imo', 'publication', 'suffering', 'karabag', 'guns', 'because', 'enough', 'next', 'jefferson', 'kirlian', 'pomposity', 'shoot', 'industrialist', 'tastes', 'hut', 'disarmed', 'lies', 'nights', 'the', 'across', 'of', 'illustration', 'idea', 'place', 'flew', '1876', 'can', 'stand', 'wilt', 'threat', 'resistance', '1895', 'tasking', 'honestly', 'grade', '600', 'strong', 'bearing', 'devils', 'original', 'then', 'looking', 'remain', 'turkish', 'turkic', 'ati', 'amendment', 'last', 'lamb', 'versed', 'discuss', 'anyone', 'interjection', 'every', '0', 'continue', 'spirit', 'has', 'westcott', 'showed', 'imagine', 'requesting', 'concretize', 'congress', 'this', 'on', 'quid', 'hypothesize', 'seemed', 'klein', 'system', 'killed', '_turkey', 'appeal', 'jvc', 'initiations', 'second', 'rex', 'old', 'should', 'easy', 'journalist', 'argue', 'massacre', 'info', 'states', 'gnostic', 'survival', 'met', 'talk', 'calendar', 'not', 'significance', 'authority', 'direct', 'fo', 'unusual', 'policy', 'confines', 'protect', 'age', 'possibly', '30', 'faltering', 'ways', 'strongly', 'criticized', 'platen', 'taken', 'normally', 'abandoned', 'commands', 'sweden', 'nj', 'social', 'impressed', 'regarding', 'urge', 'feel', 'furthermore', 'known', 'dribble', 'sexual', 'many', 'way', 'video', 'padover', 'adepts', 'said', 'o', 'tortured', 'him', 'paragraph', 'write', 'front', 'couple', 'cal', 'at', 'thomas', 'whoever', 'against', 'constitution', 'sec', 'putting', 'eventually', 'itself', 'balkans', 'arcane', 'accident', 'including', '3', 'paint', 'law_', 'section', 'ideas', 'his', 'dogmatic', 'up', 'thought', '500kbit', 'end', 'countersteering', 'government', 'understand', 'rec', 'troops', 'frees', 'franz', 'rule', 'geneva', 'solution', 'shows', 'stars', 'perry', 'short', 'already', '0055', 'mag', 'a', 'looked', 'thousands', 'hartmann', 'miller', 's', 'visual', 'them', 'their', 'laid', 'jersey', 'empire', 'support', '214', 'afternoon', 'heavily', 'feasts', 'perhaps', 'sf', 'figures', '15', 'modify', 'fear', 'disks', 'its', 'diamond', 'forgot', 'beckup', 'webster', 'followed', 'pitt', 'ancient', 'before', 'fervor', 'utility', 'shearson', 'europe', 'pulp', 'vapor', 'speaking', 'court', 'basic', 'moral', 'sci', 'bad', 'allow', 'berlin', 'holocaust', 'if', 'we', 'planes', 'cold', 'include', 'province', 'current', 'detach', 'seeked', 'principles', 'workers', 'ad', 'wynn', 'marx', 'east', 'good', 'infusing', 'ordained', 'following', 'so', 'theodore', 'line', 'outer', 'ok', 'back', 'encompass', 'feeble', 'tech', 'unfolded', 'john', 't', 'are', '27', 'authorization', 'bashers', 'associated', 'personel', 'please', 'practices', 'la', 'pretense', 'it', 'containing', 'reuss', 'invoked', 'body', 'allowed', 'swedish', 'carefully', 'orchid', 'opinion', 'watching', 'pan', 'bruins', 'assisted', 'proposal', 'motor', 'feet', 'multi', 'laughable', 'until', 'boyd', 'eyes', 'noah', 'skeptics', 'got', 'singed', 'well', 'in', 'relief', 'adult', 'us', '2', 'problem', 'lewis', 'instead', 'truth', 'request', 'nobody', '17', 'served', 'ago', 'development', 'processes', 'disagree', 'intelligent', 'fact', 'above', 'eruption', 'beyond', 'information', 'data', 'same', 'held', 'rate', 'bunch', 'acting', 'militia', 'lab', 'restriction', 'fifty', 'made', 'word', 'animals', 'lower', 'subcommittee', 'succeeded', 'revise', '250kbit', 'lie', 'kingdom', 'armenia', 'wishing', 'testimony', 'azeri', 'those', 'differently', 'brash', 'militias', 'between', 'called', 'start', 'initiated', 'fair', 'isn', 'defines', 'demirel', 'question', 'crap', 'kid', 'come', 'bought', 'resolve', 'judiciary', 'fans', 'contact', 'feature', '241', 'grey', 'gone', 'been', 'rites', 'sword', 'service', 'ram', 'member', '_the', 'azerbadjan', 'imaging', 'mechanism', 'probably', 'time', '_an', 'even', 'dthe', 'low', 'tyrants', 'another', 'locking', 'shotgun', 'behavoir', 'set', 'changed', 'rituals', 'rather', 'relay', 'under', 'requirement', 'one', 'ever', 'belong', 'retain', 'haven', 'actually', 'calibre', 'superior', 'replied', 'dream', 'surely', 'sic', 'english', 'guess', 'joining', 'noise', 'to', 'different', 'max', 'great', 'resort', 'market', 'whole', 'masonic', 'exercising', 'supports', 'rightfully', 'graphics', 'standing', 'crowley', 'greater', 'china', 'prosecution', 'seen', 'all', 'perspectives', 'skim', 'rationale', 'usa', 'started', 'fight', 'edited', 'state', 'premier', 'wanting', 'half', 'patriots', 'present', 'says', 'do', 'things', 'edition', 'milliseconds', 'individuals', '1922', 'oto', 'bait', 'spoken', 'sort', 'into', 'vector', 'ask', 'themselves', 'trying', 'leads', 'bit', 'typically', 'invoke', 'like', 'm', 'model', 'safety', 'nice', 'sea', 'any', 'leading', 'actively', 'attack', 'tlu', 'irrespective', 'initiation', 'cultures', 'eyebrows', 'galacticentric', 'redcross', 'papus', 'politics', 'although', 'send', 'say', 'let', 'these', 'perception', 'take', 'p', 'compatability', 'peoples', 'hymenaeus', 'hopefully', '200th', 'beat', 'common', 'detailed', 'pure', 'man', 'decreasing', 'orientis', 'sometimes', 'others', 'wow', 'school', 'odd', 'abc', 'important', 'brother', 'sp', 'interpret', 'kent', 'art', 'cheers', 'depending', 'sacrifices', 'your', 'bay', 'look', 'published', 'being', 'wording', 'veritatem', 'welcome', '1939', 'virginia', 'secrets', 'majestic', 'lore', 'right', 'raped', 'lock', 'over', 'posts', 'device', 'jagr', 'excellent', 'law', 'someone', 'became', 'cation', 'shall', 'cryptography', 'democracy', 'mon', 'examine', 'become', 'ounce', 'i', 'as', 'c', 'stuff', 'attached', 'going', 'experimental', 'committee', 'gyroscopes', 'j', 'rudolph', '1990', 'inescapable', 'content', 'announced', 'ide', 'police', 'revelation', 'properties', 'email', 'upwards', 'remember', 'am', 'with', 'h', 'inexpensive', 'only', 'cover', 'put', 'beware', 'yet', 'april', 'steve', 'ohhhh', 'type', 'torture', 'swath', '9', '1902', 'psychologists', 'stretch', 'pictures', 'basically', 'usual', 'report', 'several', 'perspective', 'new', 'farenheit', 'disappointed', 'vis', 'three', 'qualified', 'tried', 'bands', 'mentioned', 'liberties', 'avoid', 'rejigged', 'promulgating', 'room', 'than', 'manner', 'limits', 'comprised', 'some', 'almost', 'skills', 'behind', 'geniuses', 'etc', 'vouch', 'leaves', 'historic', 'indeed', 'seeking', 'caucasus', 'most', 'order', 'interpretations', 'papers', 'peaceful', 'riding', 'templi', 'concert', 'perpetually', 'how', 'words', 'armed', 'frater', 'sure', '1850', 'efforts', 'metzger', 'bruin', 'political', 'head', 'armenians', 'casserole', 'merlinus', 'turned', 'pamphlets', 'possible', 'sabre', 'cbc', 'thank', 'each', 'thou', 'vision', 'ford', 'named', 'tapes', 'ankara', 'myself', 'certainly', 'millisecond', 'chartered', 'licence', '1787', 'experimenters', 'want', 'find', 'intuition', 'stroke', 'recording', 'lot', 'lose', 'canada', 'military', 'killings', 'essentially', 'ix', 'transfered', 'now', 'telecasts', 'character', 'prior', 'know', 'very', 'hucksters', 'rear', 'women', 'fun', 've', 'lack', 'mean', 'smith', 'around', 'could', 'vesa', 'insist', 'german', 'afresh', 'sin', 'repealed', 'believe', 'down', '20ms', 'counted', 'flaming', 'ones', 'fill', 'chemist', 'is', 'extremes', 'impartial', 'after', 'able', 'explosive', 'knowledge', 'dma', 'moments', 'however', 'fooling', 'refresh', 'whose', 'pretty', 'spy', 'follow', 'about', 'february', 'having', 'private', '930418', 'since', 'blood', 'dawn', 'mizraim', 'computer', 'caliph', 'weight', '1950', 'season', '1', 'teeth', 'born', 'matter', 'either', 'finally', 'thanks', 'capable', 'calls', 'testing', 'house', 'day', 'think', 'cannot', 'yesterday', 'requires', 'pizza', 'difference', 'interpreted', 'little', 'kinship', 'ends', '6060', 'encompassed', 'might', '_', 'he', 'courses', 'our', 'united', 'comment', 'attempt', 'she', 'there', 'buff', 'modern', 'high', 'anyway', 'didn', 'carry', '1888', 'upon', 'individual', 'arise', 'used', 'technique', 'four', '_equinox', 'ready', 'years', 'by', 'helping', 'centralization', 'translated', 'indicates', 'anything', 'both', 'longer', 'later', 'happily', 'thelema', 'were', 'transfers', 'game', 'buit', 'example', 'william', 'entered', 'watch', 'ohhh', 'bos', 'ozal', 'leadership', 'tyranny', 'split', 'develop', 'gain', 'vlb', 'better', 'issue', 'playoffs', 'suggest', 'v', 'occurred', 'integrity', 'was', 'jubilee', 'an', 'soft', 'arms', 'jesus', 'kerb', 'rulers', 'answers', 'why', 'quite', 'golden', 'pmetzger', 'june', 'traveled', 'sanctity', 'wall', 'real', 'declares', 'get', 'non', 'and', 'they', 'orders', 'still', 'education', 'within', 'meeting', 'larger', 'code', 'official', 'limited', 'examination', 'supplied', 'dallas', 'successfully', 'founded', 'post', 'christian', 'select', '300', 'no', 'where', 'historical', 'wd40', 'mass', 'debarred', 'universal', 'based', 'widely', 'pittsburghers', 'though', 'magick', 'experiments', 'supreme', 'aeon', 'using', 'men', 'try', 'designed', 'imho', 'daughter', 'relieved', 'power', 'stealth', 'butter', 'magical', 'quoting', 'use', 'mediterranean', 'squirted', 'found', 'decision', 'recent', 'when', 'contains', 'reduce', 'yarker', 'heart', 'may', 'picture', 'driver', 'show', 'leftover', 'successor', 'u', 'kellner', 'world', 'scsi', 'hockey', 'make', 'person', 'suleyman', 'america', 'disk', 'supportive', 'first', 'notion', 'here', 'away', 'stop', 'plastic', 'preamble', 'introduced', 'much', 'earlier', 'see', 'point', 'sawed', 'citizen', 'unjust', 'boasts', 'conclusion', 'prussian', 'b', '2mb', 'july', 'instructed', 'love', 'quote', 'techmar', 'material', '1912', '1925', 'king', 'w', 'included', 'out', 'bbc', 'publishing', 'wolf', 'germany', 'safe', 'holy', 'amount', 'possession', 'samuel', 'personal', 'cried', 'interpretation', 'performance', 'questioned', 'interesting', 'modernism', 'turkey', 'who', 'courage', 'shoulder', 'established', 'difficult', 'involved', '1280', 'also', 'hard', 'mdp', 'you', 'broke', 'continuity', 'doesn', 'talking', 'formatters', '150', 'ae', 'btw', 'adhere', 'federal', 'unfortunately', 'final', 'steady', 'armenian', 'l', 'purchased', 'free', 'move', 'something', 'warned', 'pro', 'numbers', 'lets', 'influence', 'laundry', 'stated', 'physically', 'wrote', 'ratifi', 'iii', 'stats', 'pay', 'history', 'contacts', 'enforce', 'force', 'plane', 'which', 'program', '10', 'hmm', 'my', 'local', '40', 'bet', 'play', 'revealing', 'uriquidez', 'will', 'labs', 'shackled', 'strongest', 'facist', 'proclaimed', 'commentaters', 'would', 'turks', '32', 'true', 'reason', 'sanctissimus', 'participating', 'members', 'term', 'given', 'refreshed', 'freedom', 'maybe', 'throughly', 'woefully', 'pre', 'response', 'me', 'rates', 'that', 'children', 'commercial', 'value', 'socialist', 'constitute', 'taught', 'spencer', 'derived', 'senate', 'envisioned', 'freezes', 'wasn', 'france', 'too', 'killing', 'transfer', 'forth', 'league', 'refering', 'heliocentric', 'memphis', 'victim', 'islanders', 'bread', 'tree', 'therefore', 'build', 'ride', 'off', 'thrown', 'secret', 'attacks', 'glass', '600rpm', 'did', 'discussing', 'had', 'corner', 'experienced', 'full', 'don', 'empted', 'commentator', 'preservation', 'regional', 'rider', 'upsate', 'devices', '28', 'employed', 'summus', 'appear', '1776', '334', 'protected', 'death', 'gotten', 'truelove', 'pretend', 'search', 'century', 'argument', 'role_', 'have', 'convention', 'magic', 'from', '1855', 'applied', 'supply', 'husband', 'justified', 'males', 'killer', 'faq', 'series', '1982', 'expression', 'be', 'appears', 'possibilities', 'goes', 'dirty', '4'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    " \n",
    "# --- Tokenizer Function ---\n",
    "def tokenize(text):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return tokens\n",
    " \n",
    "# --- Parameters ---\n",
    "num_files = 20  # Number of files to process\n",
    " \n",
    "# --- Process Files ---\n",
    "print(f\"\\n--- TOKENIZING FIRST {num_files} FILES FROM '{input_dir}' ---\\n\")\n",
    " \n",
    "# List and sort .txt files in the directory\n",
    "all_txt_files = sorted([f for f in os.listdir(input_dir) if f.endswith(\".txt\")])\n",
    "all_tokens = set()\n",
    "# Limit to the first `num_files`\n",
    "for i, filename in enumerate(all_txt_files[:num_files], start=1):\n",
    "    filepath = os.path.join(input_dir, filename)\n",
    "    # Read file content\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    " \n",
    "    # Tokenize\n",
    "    tokens = tokenize(text)\n",
    "    # Add unique tokens to the set\n",
    "    all_tokens.update(tokens)\n",
    "\n",
    "print(all_tokens)  # Preview first 20 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed76ec",
   "metadata": {},
   "source": [
    "## ðŸ” Step 3: Normalization Pipeline (Stemming, Stop Word Removal, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f277a0d",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ—£ Instructor Talking Point:\n",
    "> Now we normalize tokens: convert to lowercase, remove stop words, apply stemming or affix stripping. This reduces redundancy and enhances search accuracy.\n",
    "\n",
    "### ðŸ”§ Your Task:\n",
    "- Use `nltk` to remove stopwords and apply stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66ae9a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like', 'aid', 'group', 'bear', 'matt', 'speed', 'memori', 'case', 'break', 'price', 'consecut', 'clever', 'untrustworthi', 'accus', '00', 'argument', 'firearm', 'far', 'must', 'russian']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\xiong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def normalize_tokens(tokens):\n",
    "    return [stemmer.stem(t) for t in tokens if t not in stop_words]\n",
    "\n",
    "# Example: normalize one document\n",
    "norm_tokens = normalize_tokens(all_tokens)\n",
    "print(norm_tokens[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34cf58",
   "metadata": {},
   "source": [
    "## ðŸ” Step 4: Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c39dd",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ—£ Instructor Talking Point:\n",
    "> We now map each normalized token to the list of document IDs in which it appears. This is the core structure that allows fast Boolean and phrase queries.\n",
    "\n",
    "### ðŸ”§ Your Task:\n",
    "- Build the inverted index using a dictionary.\n",
    "- Add code to support phrase queries using positional indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ca8f106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sure': [1, 5, 17], 'basher': [1], 'pen': [1], 'fan': [1], 'pretti': [1], 'confus': [1, 14], 'lack': [1], 'kind': [1, 11], 'post': [1, 3, 5, 12], 'recent': [1]}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_inverted_index(documents):\n",
    "    index = defaultdict(list)\n",
    "    for doc_id, text in enumerate(documents):\n",
    "        tokens = normalize_tokens(tokenize(text))\n",
    "        seen = set()\n",
    "        for token in tokens:\n",
    "            if token not in seen:\n",
    "                index[token].append(doc_id + 1)\n",
    "                seen.add(token)\n",
    "    return index\n",
    "\n",
    "inverted_index = build_inverted_index(documents)\n",
    "print(dict(list(inverted_index.items())[:10]))  # Preview first 10 terms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef4df8",
   "metadata": {},
   "source": [
    "## ðŸ§ª Test: Phrase Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db832216",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ—£ Instructor Talking Point:\n",
    "> A phrase query requires the exact sequence of terms (e.g., \"machine learning\"). To support this, extend the inverted index to store positions, not just docIDs.\n",
    "\n",
    "### ðŸ”§ Your Task:\n",
    "- Implement 2 phrase queries.\n",
    "- Demonstrate that they return the correct documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97132fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for phrase query implementation\n",
    "# You may build a position-aware index or use string search within docs after normalization\n",
    "def phrase_query(inverted_index, phrase, stemmer):\n",
    "    tokens = [stemmer.stem(w) for w in phrase.strip().split()]\n",
    "    if not tokens:\n",
    "        return set()\n",
    "    \n",
    "    # Get doc sets for each token\n",
    "    doc_sets = []\n",
    "    for token in tokens:\n",
    "        if token not in inverted_index:\n",
    "            return set()\n",
    "        doc_sets.append(set(inverted_index[token]))\n",
    "    \n",
    "    # Return intersection\n",
    "    return set.intersection(*doc_sets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3a7897d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase found in: {1, 5, 17}\n",
      "Phrase found in: {16, 6, 15}\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "query1 = \"sure\"\n",
    "docs = phrase_query(inverted_index, query1, stemmer)\n",
    "print(\"Phrase found in:\", docs)\n",
    "\n",
    "query2 = \"devices\"\n",
    "docs = phrase_query(inverted_index, query2, stemmer)\n",
    "print(\"Phrase found in:\", docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
