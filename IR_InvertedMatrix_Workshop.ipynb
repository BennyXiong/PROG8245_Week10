{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890288b1",
   "metadata": {},
   "source": [
    "# 🛠️ Active Learning Workshop: Implementing an Inverted Matrix (Jupyter + GitHub Edition)\n",
    "## 🔍 Workshop Theme\n",
    "*Readable, correct, and collaboratively reviewed code—just like in the real world.*\n",
    "### Team:\n",
    "- **Zhimin Xiong** \n",
    "- **Yu-Chen Chou**\n",
    "- **Haysam Elamin**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e78d18",
   "metadata": {},
   "source": [
    "\n",
    "Welcome to the 90-minute workshop! In this hands-on session, your team will build an **Inverted Index** pipeline, the foundation of many intelligent systems that need fast and relevant access to text data — such as AI agents.\n",
    "\n",
    "### 👥 Team Guidelines\n",
    "- Work in teams of 3.\n",
    "- Submit one completed Jupyter Notebook per team.\n",
    "- The final notebook must contain **Markdown explanations** and **Python code**.\n",
    "- Push your notebook to GitHub and share the `.git` link before class ends.\n",
    "\n",
    "---\n",
    "## 🔧 Workshop Tasks Overview\n",
    "\n",
    "1. **Document Collection**\n",
    "2. **Tokenizer Implementation**\n",
    "3. **Normalization Pipeline (Stemming, Stop Words, etc.)**\n",
    "4. **Build and Query the Inverted Index**\n",
    "\n",
    "> Each step includes a sample **talking point**. Your team must add your own custom **Markdown + code cells** with a **second talking point**, and test your Inverted Index with **2 phrase queries**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a922333",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 🧠 Learning Objectives\n",
    "- Implement an **Inverted Matrix** using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into peer notebooks.\n",
    "- Practice **collaborative debugging**, professional peer feedback, and improve code quality.\n",
    "\n",
    "## 🧩 Workshop Structure (90 Minutes)\n",
    "1. **Instructor Use Case Introduction** *(15 min)* – Set up teams of 3 people. Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Team Jupyter Notebook Development** *(45 min)* – Manual IR and Inverted Matrix coding + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** *(15 min)* – Teams commit and push initial notebooks. **Make sure to include your names so it is easy to identify the team that developed the Min-Max code**.\n",
    "4. **Instructor Review** - The instructor will go around, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "5. **Email Delivery** *(15 min)* – Each team send the instructor an email **with the *.git link** to the GitHub repo **(one email/team)**. Subject on the email is: PROG8245 - Inverted Matrix  Workshop, Team #_____.\n",
    "\n",
    "\n",
    "## 💻 Submission Checklist\n",
    "- ✅ `IR_InvertedMatrix_Workshop.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline, and Inverted Index.\n",
    "  - Markdown explanations for each major step\n",
    "  - **Labeled talking point(s)** and 2 phrase query tests\n",
    "- ✅ `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- ✅ GitHub Repo:\n",
    "  - Public repo named `IR-invertedmatrix-workshop`\n",
    "  - This is a group effort, so **choose one member of the team** to publish the repo\n",
    "  - At least **one commit containing one meaningful talking point**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e160c9d",
   "metadata": {},
   "source": [
    "## 📄 Step 1: Document Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc964464",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> We begin by gathering a text corpus. To build a robust index, your vocabulary should include **over 2000 unique words**. You can use scraped articles, academic papers, or open datasets.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Collect at least 20+ text documents.\n",
    "- Ensure the vocabulary exceeds 2000 unique words.\n",
    "- Load the documents into a list for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ee0c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 documents.\n"
     ]
    }
   ],
   "source": [
    "# Example: Load text files from a folder\n",
    "import os\n",
    "\n",
    "input_dir = 'sample_docs/'\n",
    "\n",
    "def load_documents(folder_path):\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                documents.append(file.read())\n",
    "    return documents\n",
    "\n",
    "# Replace 'sample_docs/' with your actual folder\n",
    "documents = load_documents(input_dir)\n",
    "print(f\"Loaded {len(documents)} documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b342945a",
   "metadata": {},
   "source": [
    "## ✂️ Step 2: Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2803fb52",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> The tokenizer breaks raw text into a stream of words (tokens). This is the foundation for every later step in IR and NLP.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Implement a basic tokenizer that splits text into lowercase words.\n",
    "- Handle punctuation removal and basic non-alphanumeric filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7c3654e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TOKENIZING FIRST 20 FILES FROM 'sample_docs/' ---\n",
      "\n",
      "{'impressed', 'area', 'mass', 'killing', 'body', 'empire', 'successor', 'disappointed', 'promulgating', 'concert', 'commentaters', 'his', 'pamphlets', 'america', 'means', 'talk', 'gyroscopes', 'world', 'illustration', 'webster', 'person', 'vlb', 'edited', 'support', 'solution', 'born', 'pmetzger', 'split', 'response', 'two', 'turkish', 'bashers', 'resigned', 'writing', 'farenheit', 'an', 'fooling', 'appointed', 'maybe', 'role_', 'thelema', 'cried', 'corner', 'premier', 'normally', 'between', 'indeed', 'ready', 'over', 'quoting', 'empted', 'requirement', 'related', 'organization', 'acting', 'butter', 'various', 'fervor', 'deeply', 'until', 'rulers', 'supply', 'wall', 'on', 'lamb', 'personel', 'leaves', 'declares', 'historical', 'didn', 'carefully', 'named', 'tree', 'had', '5', 'shoulder', 'where', 'warned', 'known', 'apprehend', 'believes', 'self', 'telecasts', 'stroke', 'interpretations', 'abandoned', 'hmm', 'both', 'skim', 'full', 'oto', 'revelation', 'historic', 'going', 'issue', 'facist', 'rear', 'across', 'difficult', 'longstanding', 'recording', 'sort', 'sp', 'testimony', 'lewis', 'protect', 'old', 'second', 'following', 'thus', 'price', 'task', 'announced', 'radius', 'seemed', 'instructed', 'revise', 'sanctissimus', 'islanders', 'reuss', 'la', 'private', 'ride', 'value', 'term', 'hearded', 'kurds', 'protects', 'specific', 'search', 'edition', '500kbit', 'guess', 'feature', 'couple', '120kvolt', 'mizraim', 'brake', 'matt', 'transfered', 'majestic', 'included', '1', 'metzger', 'off', 'a', 'freedom', 'giving', 'ix', 'now', 'aura', 'direct', 'fans', 'argue', 'constitute', 'mean', 'moments', 'riding', 'suggestions', 'confines', '97th', 'including', 'goes', 'decision', '1895', 'assisted', 'sacrifices', 'tasking', 'calendar', 'mason', 'putting', 'section', 'truth', 'once', 'adult', 'looking', 'attached', 'prussian', 'initiations', 'socialist', 'explicitly', 'publish', 'franz', 'february', 'interpret', 'believe', 'wasn', 'pulp', 'galacticentric', 'league', 'front', 'greater', 'understanding', 'police', 'good', 'high', 'animals', 'stretching', 'back', 'throughly', 'veritatem', 'lock', 'parts', 'torture', 'audience', 'turkic', '_equinox', 'nights', 'bad', 'fair', 'vector', 'place', 'represent', 'bos', 'basic', 'skeptics', 'ok', 'debarred', 'grade', 'perpetually', 'build', 'azeri', 'psychologists', 'beyond', 'upon', 'often', '250kbit', 'starting', 'anyway', 'mdp', 'christmas', 'less', 'divinity', 'commercial', 'defense', 'set', 'karabag', 'principle', 'rapings', 'refering', 'within', 'fun', 'model', 'reasonably', 'enforce', '1912', 'cation', 'bought', 'there', 'plastic', 'perry', 'jasmine', 'yesterday', 'superior', 'cbc', 'depending', 'countersteer', 'question', 'assess', 'able', '6060', 'wish', 'thrown', 'not', 'technique', 'diamond', 'worse', 'mediterranean', 'worked', 'attack', 'word', 'quite', 'permit', 'important', 'disks', 'they', 'infiltrating', 'resort', 'days', 'original', 'way', 'experiments', 'longer', 'virginia', 'ati', 'rev', 'of', 'why', 'such', 'tastes', 'sanctity', 'above', 'principles', 'east', 'purchased', 'fill', 'something', 'ends', 've', 'how', 'possess', 'containing', 'grey', 'doesn', 'either', 'h', 'secret', 'bruins', 'pens', 'examiner', 'our', 'smith', 'spirit', 'would', 'bbc', 'outer', 'tyranny', 'published', 'first', 'china', 'vesa', 'berlin', 'sword', '1950', 'turned', 'frees', 'health', 'o', 'puzzled', 'made', 'gain', 'platen', 'rpm', 'relieved', 'stuff', 'preamble', 'ask', 'spy', 'continue', 'augsburg', 'yeah', 'political', 'blood', 'bmf', 'since', '1787', '1922', 'closely', 'fight', 'ancient', '1939', '600rpm', '2', 'utility', 'info', 'peoples', 'themselves', 'usual', 'interpretation', 'policy', 'tlu', 'ideas', 'away', 'from', 'laws', 'schooled', '930418', 'feeble', 'whoever', 'settled', 'it', 'successfully', 'clever', 'finally', 'feel', 'or', 'information', 'weiser', 'steady', 'employed', 'ever', 'we', 'democracy', 'used', 'three', 'performance', 'ide', 'free', 'character', 'seems', '1888', 'guns', 'europe', 'liberties', 'day', 'nj', 'christian', 'sexual', 'people', 'magick', 'half', 'has', 'fascist', 'line', 'groups', 'defines', '0', 'bearing', 'lack', '1876', 'marx', 'down', 'constitution', 'masonic', 'together', 'under', 'show', 'my', 'tyrants', 'regarding', 'argumentation', 'keep', 'leadership', '1982', 'whose', 'rate', 'seen', 'bay', '8', 'militia', 'azeris', 'accepted', 'motor', 'patriots', 'contacts', 'actually', 'answers', 'development', 'dawn', 'other', 'displays', 'paint', 'kirlian', 'cold', 'book', 'resistance', 'ap', 'ultra', 'pretense', 'inexpensive', 'teeth', 'them', 'rec', 'jesus', 'basically', 'bruin', 'ago', 'four', 'computers', 'unfortunately', 'applied', 'found', 'magical', 'subcommittee', 'some', 'he', 'end', 'many', 'redcross', 'united', 'kind', 'infusing', 'limited', 'prosecution', 'eye', 'bell', 'doing', 'envisioned', 'canada', 'let', 'arise', 'dribble', 'que', 'swedish', 'what', '1905', 'ae', '1990', 'test', 'sabre', 'squirted', 'labs', 'transfers', 'playoffs', 'thousands', 'doubts', 'manner', 'azerbadjan', '6', 'thank', 'few', 'translated', 'figures', 'season', 'designed', 'bait', 'rejigged', 'bet', 'house', '28', 'remember', 'changed', 'introduction', 'short', 'nobody', 'karl', 'vision', 'aid', 'brother', 'lies', 'thou', 'perspective', 'citizen', 'her', 'go', 'work', 'explanations', 'every', 'new', 'eyebrows', 'countersteering', 'kingdom', 'report', 'publishing', 'permitted', 'too', 'by', 'armed', 'children', 's', 'uerdugo', 'stats', 'ohhhh', 'lie', '_', 'after', 'don', 'trying', 'member', 'forth', 'however', 'influence', 'present', 'series', 'westcott', 'suleyman', 'commentator', 'pomposity', 'pre', 'enough', 'involved', 'broke', 'speed', 'men', 'hopefully', 'multi', 'spectacular', 'males', 'idea', 'orientis', '1925', 'does', 'fifty', 'drive', 'bread', 'carry', '214', 'jagr', 'raised', 'supplied', 'steve', '7', 'fear', 'heliocentric', 'discuss', 'appear', 'preservation', 'participating', 'these', 'padover', 'firearms', 'likely', 'flew', 'universal', 'pass', 'content', 'heinrich', '10', 'victim', 'about', 'surface', 'gladly', 'graphics', 'max', 'which', 'called', 'letter', 'be', 'force', 'quote', 'conclusion', 'says', 'common', 'flaming', 'techmar', 'laughable', 'rather', 'entered', 'faq', 'knew', 'milliseconds', 'province', 'furthermore', 'hucksters', 'just', 'that', 'm', 'bunch', 'intuition', 'government', 'myself', 'sometimes', '1902', 'courses', 'sure', 'but', 'adhere', 'probably', 'picture', 'showed', 'guidelines', 'turkey', 'joining', 'market', 'continuity', 'commands', 'chemist', '1850', 'isn', 'order', 'mentioned', 'eventually', 'modernism', 'started', 'might', 'give', 'vouch', 'still', 'wording', 'mag', 'will', 'here', 'capable', 'interesting', 'posts', 'confused', 'very', 'welcome', 'holy', 'shearson', 'so', 'itself', 'therefore', 'one', 'individuals', 'noise', 'different', 'turanist', 'extend', 'meeting', '10_', 'geniuses', 'weapons', 'samuel', 'visual', 'supportive', 'requesting', 'stand', 'each', 'kent', 'safe', 'licence', 'interjection', 'april', 'ones', 'john', 'occur', 'death', 'critical', 'like', 'dthe', 'restriction', 'better', 'sweden', 'circle', 'threat', 'you', 'pretend', 'opinion', 'committee', 'husband', 'plane', 'modern', 'exercising', 'avoid', 'use', 'convention', 'testing', 'casserole', 'differently', 'rex', 'thanks', 'before', 'contains', 'heavily', 'laid', '17', 'hypothesize', 'supports', 'locking', 'armenia', 'faltering', 'example', 'shows', 'suggest', 'wow', 'stretch', 'this', 'insist', 'contact', 'adepts', 'significance', 'established', 'comment', 'leads', 'assistant', 'talking', 'l', 'hard', 'context', 'traveled', 'hymenaeus', 'law', '15', 'and', 'theodore', 'calls', 'send', 'make', '20', 'device', 'busy', 'transfer', 'paper', 'around', 'service', 'widely', 'chartered', 'daughter', 'current', 'forgot', 'unjust', 'serious', 'later', 'ability', 'encompassed', 'gnostic', 'whole', 'ounce', 'wd40', 'concretize', 'expected', 'killer', 'possession', 'stars', 'refresh', 'much', 'truelove', 'request', 'upsate', 'sea', 'kid', 'bands', 'express', 'skills', 'shotgun', 'strongly', 'criticized', 'sic', 'were', 'ordained', 'geneva', 'break', 'appeal', 'boasts', 'cover', 'though', 'although', 'us', 'power', 'bowman', 'put', 'c', 'proclaimed', 'buff', 'kerb', 'pro', 'armenians', 'room', 'while', 'held', 'age', 'federal', 'afternoon', 'can', 'education', 'repealed', 'through', 'out', 'crap', 'notion', 'beta', 'allow', 'email', 'already', 'golden', 'freezes', 'no', '9', 'artists', 'wants', 'rule', 'tried', 'pittsburghers', 'kinship', 'feasts', 'man', 'expression', 'spencer', 'refreshed', 'singed', 'wynn', 'rituals', 'disarmed', 'decreasing', 'resolve', 'crowley', 'when', '20ms', 'peaceful', 'anyone', 'comprised', 'me', 'orchid', 'memory', 'summus', 'journalist', 'nice', 'perhaps', 'follow', 'j', 'feet', 'yarker', 'official', 'presented', 'your', 'bus', 'its', 'sci', 'court', 'killed', 'to', 'pictures', 'regional', 'computer', 'behavoir', 'caliph', 'extremes', 'sec', 'their', 'all', 'june', 'even', 'was', 'states', 'jvc', 'appears', 'imaging', 'think', 'gotten', 'if', 'introduced', 'look', 'write', 'at', 'individual', 'courage', 'tsn', '_turkey', 'against', 'matter', 'weight', '32', 'problem', 'examination', 'steiner', 'see', 'growth', 'amount', 'speaking', 'preserve', 'belong', 'connect', 'liberty', 'rates', 'founded', 'others', 'given', 'jefferson', 'fact', 'ad', 'simplistic', 'excellent', 'hut', 'lose', 'tapes', 'happily', 'stated', 'nothing', 'gone', 'almost', 'detach', 'u', 'aeon', 'based', 'lets', 'well', 'low', 'than', 'singer', 'ratifi', 'watch', 'certainly', 'wealthy', 'say', 'earlier', 'magic', 'said', 'examine', 'take', 'helping', 'mechanism', 'cryptography', 'program', 'russian', 'point', 'remain', 'papus', 'vapor', 'disagree', 'honestly', 'session', 'ed', 'replied', 'dallas', 'tv', 'imho', 'shoot', 'france', 'derived', 'interpreted', 'unusual', 'system', 'cultures', 'gic', 'only', 'politics', 'games', 'years', 'arms', 'shackled', 'btw', 'pizza', 'having', 'stands', 'relay', 'eruption', 'wilt', 'none', 'consecutive', 'ozal', 'quid', 'select', 'modify', 'because', 'usa', 'abc', 'upwards', 'as', 'time', 'caucasus', 'pan', 'possibly', 'deleted', 'up', 'inescapable', 'become', 'afresh', 'easy', '1855', 'century', 'glass', 'army', 'essentially', 'driver', 'please', 'data', 'w', 'dma', 'masonry', 'conditions', '3', 'being', 'imagine', 'detailed', 'also', 'real', '600', 'video', 'could', 'include', 'orders', 'pure', 'devils', 'troops', 'is', 'boyd', 'wanting', 'law_', 'initiation', '334', 'experimental', 'impartial', 'templi', 'unfolded', 'perspectives', 'school', 'militias', 'july', 'same', 'anything', 'germany', 'spoken', 'secrets', 'kellner', 'revealing', 'questioned', 'os', 'turks', 'physically', 'dogmatic', 'for', 'irrespective', 'him', 'millisecond', 'william', 'got', '200th', 'english', 'judiciary', 'seeked', 'compatability', 'experimenters', 'retain', 'centralization', 'get', 'she', 'did', 'initiated', 'lower', 'members', 'fox', 'amendment', 'odd', 'buit', 'components', 'wrote', 'wishing', 'far', 'senate', 'moral', 'lore', 'publication', 'arcane', 'disk', 'woefully', 'counted', 'authorization', 'frater', 'difference', 'encompass', 'mighty', 'been', 'heart', 'strongest', 'workers', 'aquire', 'using', 'last', 'leftover', 'complex', '_an', 'numbers', '1993', 'beat', 'processes', '4', 'ram', 'equinox', 'raped', 'shall', 'succeeded', 'lab', 'massacre', 'taught', 'congress', 'indicates', 'tech', 'klein', 'followed', '1280', 'love', 'industrialist', 'choice', 'case', 'try', 'standing', 'urge', 'hartmann', 'oriflamme', 'moment', 'austrian', 'sf', 'pretty', 'teletype', 'bear', 'armenian', 'reduce', 'stealth', 'know', 'became', 'do', 'balkans', 'tortured', 'allowed', 'came', '0055', 'country', 'discussing', 'strong', '_the', 'rider', 'explosive', 'looked', 'versed', '150', 'limits', 'suffering', 'card', 'cal', 'x', '1776', 'prior', 'miller', 'possible', 'attempt', 'instead', 'experienced', 'iii', 'have', 'job', 'papers', 'etc', 'into', 'cannot', 'knowledge', 'formatters', 'may', 'p', 'right', 'leading', 'uriquidez', 'accused', 'side', 'rudolph', 'sin', 'phrase', 'king', 'reason', 'safety', 'leader', 'integrity', 'are', 'fo', 'tape', 'cheers', 'ordo', 'properties', '2mb', '241', 'yet', 'haven', '30', 'demirel', 'soft', 'post', 'doctrines', 'behind', 'relief', 'i', 'imo', 'served', 'rites', 'occurred', 'beware', 'major', 'come', '40', 'things', 'surely', 'swath', 'rightfully', 'lot', 'adriatic', 'develop', 'memphis', 'calibre', 'head', 'art', 'those', 'women', 'who', 'protected', 'efforts', 'the', 'wolf', 'taken', 'hockey', 'best', 'beckup', 'thought', '300', 'local', 'several', 'laundry', 'recent', 'move', 'ankara', 'explain', 'great', 'argument', 'further', 'another', 'eyes', 'invoked', 'justified', 'dirty', 'survival', 'com', 'little', 'supreme', 'history', 'thread', 'practical', 'attempts', 'type', 'social', 'brash', 'german', 'qualified', 'paragraph', 'wings', 'scsi', 'stop', 'non', 'pay', 'noah', 'more', 'should', 'words', 'possibilities', 'next', 'start', 'praise', 'bit', 'sawed', 'larger', 'someone', 'ford', 'actively', 'change', 'code', 'surprises', 'holocaust', '00', 't', 'with', 'any', 'am', 'then', 'final', 'must', 'requires', 'attacks', 'invoke', 'practices', 'associated', 'hope', 'true', 'reasons', 'play', 'takes', 'regular', 'accident', 'material', 'authority', 'background', 'find', 'merlinus', 'killings', 'watching', 'pitt', 'game', 'rationale', 'understand', 'jubilee', '27', 'untrustworthy', 'thomas', 'v', 'ohhh', 'in', 'ways', 'planes', 'most', 'personal', 'intelligent', 'manyk', 'jersey', 'typically', 'own', 'concept', 'met', 'devices', 'military', 'proposal', 'mon', 'perception', 'b', 'want', 'seeking', 'vis', 'fuhr', 'state', 'dream'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    " \n",
    "# --- Tokenizer Function ---\n",
    "def tokenize(text):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return tokens\n",
    " \n",
    "# --- Parameters ---\n",
    "num_files = 20  # Number of files to process\n",
    " \n",
    "# --- Process Files ---\n",
    "print(f\"\\n--- TOKENIZING FIRST {num_files} FILES FROM '{input_dir}' ---\\n\")\n",
    " \n",
    "# List and sort .txt files in the directory\n",
    "all_txt_files = sorted([f for f in os.listdir(input_dir) if f.endswith(\".txt\")])\n",
    "all_tokens = set()\n",
    "# Limit to the first `num_files`\n",
    "for i, filename in enumerate(all_txt_files[:num_files], start=1):\n",
    "    filepath = os.path.join(input_dir, filename)\n",
    "    # Read file content\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    " \n",
    "    # Tokenize\n",
    "    tokens = tokenize(text)\n",
    "    # Add unique tokens to the set\n",
    "    all_tokens.update(tokens)\n",
    "\n",
    "print(all_tokens)  # Preview first 20 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed76ec",
   "metadata": {},
   "source": [
    "## 🔁 Step 3: Normalization Pipeline (Stemming, Stop Word Removal, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f277a0d",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> Now we normalize tokens: convert to lowercase, remove stop words, apply stemming or affix stripping. This reduces redundancy and enhances search accuracy.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Use `nltk` to remove stopwords and apply stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66ae9a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['impress', 'area', 'mass', 'kill', 'bodi', 'empir', 'successor', 'disappoint', 'promulg', 'concert', 'commentat', 'pamphlet', 'america', 'mean', 'talk', 'gyroscop', 'world', 'illustr', 'webster', 'person']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/hsi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def normalize_tokens(tokens):\n",
    "    return [stemmer.stem(t) for t in tokens if t not in stop_words]\n",
    "\n",
    "# Example: normalize one document\n",
    "norm_tokens = normalize_tokens(all_tokens)\n",
    "print(norm_tokens[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34cf58",
   "metadata": {},
   "source": [
    "## 🔍 Step 4: Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c39dd",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> We now map each normalized token to the list of document IDs in which it appears. This is the core structure that allows fast Boolean and phrase queries.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Build the inverted index using a dictionary.\n",
    "- Add code to support phrase queries using positional indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ca8f106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yeah': [1], 'second': [1, 7, 9, 12, 17], 'one': [1, 7, 11, 13, 15, 16, 19], 'believ': [1, 4, 9], 'price': [1, 19], 'tri': [1, 6, 15, 16], 'get': [1, 11, 12, 15], 'good': [1], 'look': [1, 4, 7, 11], 'bruin': [1]}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_inverted_index(documents):\n",
    "    index = defaultdict(list)\n",
    "    for doc_id, text in enumerate(documents):\n",
    "        tokens = normalize_tokens(tokenize(text))\n",
    "        seen = set()\n",
    "        for token in tokens:\n",
    "            if token not in seen:\n",
    "                index[token].append(doc_id + 1)\n",
    "                seen.add(token)\n",
    "    return index\n",
    "\n",
    "inverted_index = build_inverted_index(documents)\n",
    "print(dict(list(inverted_index.items())[:10]))  # Preview first 10 terms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef4df8",
   "metadata": {},
   "source": [
    "## 🧪 Test: Phrase Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db832216",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> A phrase query requires the exact sequence of terms (e.g., \"machine learning\"). To support this, extend the inverted index to store positions, not just docIDs.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Implement 2 phrase queries.\n",
    "- Demonstrate that they return the correct documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97132fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for phrase query implementation\n",
    "# You may build a position-aware index or use string search within docs after normalization\n",
    "def phrase_query(inverted_index, phrase, stemmer):\n",
    "    tokens = [stemmer.stem(w) for w in phrase.strip().split()]\n",
    "    if not tokens:\n",
    "        return set()\n",
    "    \n",
    "    # Get doc sets for each token\n",
    "    doc_sets = []\n",
    "    for token in tokens:\n",
    "        if token not in inverted_index:\n",
    "            return set()\n",
    "        doc_sets.append(set(inverted_index[token]))\n",
    "    \n",
    "    # Return intersection\n",
    "    return set.intersection(*doc_sets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3a7897d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase sure found in: {18, 14, 7}\n",
      "Phrase devices found in: {16, 17, 4}\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "query1 = \"sure\"\n",
    "docs = phrase_query(inverted_index, query1, stemmer)\n",
    "print(f\"Phrase {query1} found in:\", docs)\n",
    "\n",
    "query2 = \"devices\"\n",
    "docs = phrase_query(inverted_index, query2, stemmer)\n",
    "print(f\"Phrase {query2} found in:\", docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
