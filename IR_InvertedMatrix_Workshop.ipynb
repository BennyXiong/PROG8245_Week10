{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890288b1",
   "metadata": {},
   "source": [
    "# 🛠️ Active Learning Workshop: Implementing an Inverted Matrix (Jupyter + GitHub Edition)\n",
    "## 🔍 Workshop Theme\n",
    "*Readable, correct, and collaboratively reviewed code—just like in the real world.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e78d18",
   "metadata": {},
   "source": [
    "\n",
    "Welcome to the 90-minute workshop! In this hands-on session, your team will build an **Inverted Index** pipeline, the foundation of many intelligent systems that need fast and relevant access to text data — such as AI agents.\n",
    "\n",
    "### 👥 Team Guidelines\n",
    "- Work in teams of 3.\n",
    "- Submit one completed Jupyter Notebook per team.\n",
    "- The final notebook must contain **Markdown explanations** and **Python code**.\n",
    "- Push your notebook to GitHub and share the `.git` link before class ends.\n",
    "\n",
    "---\n",
    "## 🔧 Workshop Tasks Overview\n",
    "\n",
    "1. **Document Collection**\n",
    "2. **Tokenizer Implementation**\n",
    "3. **Normalization Pipeline (Stemming, Stop Words, etc.)**\n",
    "4. **Build and Query the Inverted Index**\n",
    "\n",
    "> Each step includes a sample **talking point**. Your team must add your own custom **Markdown + code cells** with a **second talking point**, and test your Inverted Index with **2 phrase queries**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a922333",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 🧠 Learning Objectives\n",
    "- Implement an **Inverted Matrix** using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into peer notebooks.\n",
    "- Practice **collaborative debugging**, professional peer feedback, and improve code quality.\n",
    "\n",
    "## 🧩 Workshop Structure (90 Minutes)\n",
    "1. **Instructor Use Case Introduction** *(15 min)* – Set up teams of 3 people. Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Team Jupyter Notebook Development** *(45 min)* – Manual IR and Inverted Matrix coding + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** *(15 min)* – Teams commit and push initial notebooks. **Make sure to include your names so it is easy to identify the team that developed the Min-Max code**.\n",
    "4. **Instructor Review** - The instructor will go around, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "5. **Email Delivery** *(15 min)* – Each team send the instructor an email **with the *.git link** to the GitHub repo **(one email/team)**. Subject on the email is: PROG8245 - Inverted Matrix  Workshop, Team #_____.\n",
    "\n",
    "\n",
    "## 💻 Submission Checklist\n",
    "- ✅ `IR_InvertedMatrix_Workshop.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline, and Inverted Index.\n",
    "  - Markdown explanations for each major step\n",
    "  - **Labeled talking point(s)** and 2 phrase query tests\n",
    "- ✅ `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- ✅ GitHub Repo:\n",
    "  - Public repo named `IR-invertedmatrix-workshop`\n",
    "  - This is a group effort, so **choose one member of the team** to publish the repo\n",
    "  - At least **one commit containing one meaningful talking point**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e160c9d",
   "metadata": {},
   "source": [
    "## 📄 Step 1: Document Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc964464",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> We begin by gathering a text corpus. To build a robust index, your vocabulary should include **over 2000 unique words**. You can use scraped articles, academic papers, or open datasets.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Collect at least 20+ text documents.\n",
    "- Ensure the vocabulary exceeds 2000 unique words.\n",
    "- Load the documents into a list for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ee0c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 documents.\n"
     ]
    }
   ],
   "source": [
    "# Example: Load text files from a folder\n",
    "import os\n",
    "\n",
    "input_dir = 'sample_docs/'\n",
    "\n",
    "def load_documents(folder_path):\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                documents.append(file.read())\n",
    "    return documents\n",
    "\n",
    "# Replace 'sample_docs/' with your actual folder\n",
    "documents = load_documents(input_dir)\n",
    "print(f\"Loaded {len(documents)} documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b342945a",
   "metadata": {},
   "source": [
    "## ✂️ Step 2: Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2803fb52",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> The tokenizer breaks raw text into a stream of words (tokens). This is the foundation for every later step in IR and NLP.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Implement a basic tokenizer that splits text into lowercase words.\n",
    "- Handle punctuation removal and basic non-alphanumeric filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7c3654e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TOKENIZING FIRST 20 FILES FROM 'sample_docs/' ---\n",
      "\n",
      "{'related', 'actively', 'develop', '5', 'their', 'being', 'eventually', 'good', 'under', 'quoting', 'show', 'edited', 'eye', 'here', 'next', 'line', 'demirel', 'understanding', 'shall', 'irrespective', '97th', 'pretense', 'something', 'assisted', 'singed', 'beware', 'nobody', 'unusual', 'rear', 'nothing', 'certainly', 'century', 'encompass', 'weight', 'pmetzger', 'scsi', 'forth', 'armed', 'represent', 'art', 'countersteer', 'authority', 'lie', 'of', 'laughable', 'great', 'jvc', 'stuff', 'iii', 'guess', 'lock', 'fox', 'authorization', 'specific', 'including', 'constitution', 'moments', 'preamble', 'pamphlets', 'one', 'takes', 'court', 'changed', 'individual', 'finally', 'singer', 'soft', 'appeal', 'refreshed', 'half', 'bought', 'protected', 'background', 'suggest', 'would', 'fill', 'problem', 'change', 'riding', 'card', 'integrity', 'men', 'give', 'defines', 'stroke', 'rev', '600', 'honestly', 'turks', 'thanks', 'ide', 'versed', 'this', 'years', 'perception', 'feet', 'going', 'ones', '28', 'caucasus', 'for', 'front', 'these', 'explicitly', 'held', 'military', 'looked', 'came', 'ever', 'seemed', 'graphics', 'self', 'extend', 'hearded', 'recent', 'supply', 'term', 'interpret', 'uerdugo', 'countersteering', 'permitted', 'fact', 'appears', 'they', 'material', 'law', 'dawn', 'no', '1876', 'look', '334', 'america', 'others', 'spoken', 'heart', 'bos', 'peaceful', 'followed', 'task', 'plane', 'nj', 'report', 'commentator', 'bunch', 'even', 'hartmann', 'bet', 'better', 'bad', 'side', 'job', 'requirement', 'secrets', 'radius', 'chartered', 'perpetually', 'appear', 'wd40', 'mean', 'anything', 'permit', 'steve', 'spencer', 'rpm', 'judiciary', 'turkic', 'displays', 'model', 'prussian', 'until', 'preservation', 'rider', 'lies', 'split', 'galacticentric', 'relieved', 'survival', '10_', 'imagine', 'argumentation', 'fair', 'does', 'que', 'lewis', 'oriflamme', 'max', 'gain', 'kinship', 'at', 'too', 'fight', 'vouch', 'pan', 'firearms', 'sanctity', 'mentioned', 'program', 'abc', 'assistant', '300', 'surface', 'device', 'ae', 'edition', 'ends', 'rationale', 'questioned', 'dogmatic', 'although', 'centralization', 'force', 'bmf', 'watch', 'case', 'apprehend', 'place', 'army', 'historic', 'same', 'once', 'kid', 'invoke', 'possess', 'founded', 'butter', 'pretty', 'debarred', 'pitt', 'millisecond', 'wealthy', 'crowley', 'talk', 'impressed', 'none', 'thousands', 'published', 'secret', 'militia', 'presented', 'words', 'faq', '1787', 'easy', 'either', 'justified', 'value', 'attached', 'time', 'rulers', 'uriquidez', 'than', 'regarding', 'still', 'swedish', 'talking', 'caliph', 'posts', 'infusing', 'express', 'contacts', 'ordained', 'unfortunately', 'off', 'arcane', 'though', 'resort', 'mechanism', 'components', 'capable', 'orchid', 'retain', 'pens', 'final', 'explain', 'brash', 'mighty', 'h', 'come', 'pay', 'interpreted', 'league', 'milliseconds', 'named', 'start', 'eruption', 'transfers', 'dribble', 'jasmine', 'upon', 'rapings', 'traveled', 'sword', 'figures', 'political', 'strongly', 'argue', 'later', 'gnostic', 'made', 'official', 'ago', 'further', 'publishing', 'suffering', 'only', 'manyk', 'essentially', 'cold', 'concretize', 'showed', 'please', 'skim', 'surprises', 'joining', 'canada', 'sf', '17', 'modernism', 'issue', 'skills', 'almost', 'out', 'noah', 'conclusion', 'practices', 'occur', 'wishing', 'transfered', 'corner', 'l', 'hard', 'were', 'forgot', 'where', 'citizen', 'cheers', '1950', 'supreme', 'true', 'english', 'write', 'oto', 'bus', 'second', 'arise', 'you', 'sexual', 'wording', 'question', 'prior', 'age', 'motor', '40', 'pre', 'three', '20ms', 'hmm', 'could', 'os', '214', 'rather', 'worked', 'comprised', 'woefully', 'given', 'disagree', 'says', 'acting', 'guns', 'seen', 'limits', 'perhaps', 'china', 'not', 'truth', 'belong', 'superior', 'russian', 'am', '1982', 'therefore', 'tlu', 'shackled', 'sacrifices', 't', 'sometimes', 'possible', 'thought', 'jesus', 'vlb', 'france', 'putting', 'test', 'cbc', 'price', 'yesterday', 'daughter', 'ask', 'magick', 'put', 'surely', 'participating', 'expression', 'holocaust', '600rpm', 'testimony', 'tortured', 'lose', 'gone', 'christmas', 'solution', 'thread', 'orders', 'speaking', '_the', 'excellent', 'azeris', '27', 'universal', 'which', 'premier', 'introduced', 'continue', 'cation', '0', 'masonic', 'upwards', 'stats', 'tree', 'didn', 'games', 'bearing', 'beyond', 'v', 'smith', 'crap', 'don', 'find', 'usa', 'impartial', 'ratifi', 'threat', 'intelligent', 'idea', 'earlier', 'tyrants', 'allow', 'moment', 'relief', 'sea', 'karabag', 'teletype', 'german', 'reasonably', 'militias', 'leadership', 'succeeded', 'how', 'taken', 'mag', 'pretend', 'squirted', 'likely', 'keep', 'bruins', 'computer', 'templi', 'turkey', 'sci', 'request', 'vis', 'extremes', 'april', 'east', 'ordo', 'bbc', 'person', 'our', 'gic', 'teeth', 'or', 'prosecution', 'game', 'modern', 'appointed', 'disks', 'grade', 'protects', 'people', 'perspectives', 'spectacular', 'rituals', 'brake', 'role_', 'leader', 'requesting', 'individuals', 'writing', 'possibilities', 'holy', 'education', 'cal', 'examination', 'members', 'there', 'make', 'redcross', '0055', 'significance', 'shoulder', 'experimenters', 'proposal', 'support', 'four', 'discuss', 'interjection', 'served', 'amendment', 'free', 'noise', 'cannot', 'be', 'direct', 'translated', 'answers', 'attacks', 'testing', 'protect', 'playoffs', 'love', 'less', 'private', 'me', 'must', 'liberties', 'limited', 'ok', 'aquire', 'physically', 'fuhr', 'assess', 'starting', '1912', 'behavoir', 'allowed', 'adult', 'yeah', 'bait', 'well', 'doubts', 'phrase', 'last', 'heinrich', 'already', 'anyway', 'fo', 'often', '_equinox', 'liberty', 'development', 'carefully', 'schooled', 'might', 'feeble', 'many', 'declares', 'males', 'rate', 'laws', 'w', 'principles', 'successor', 'adriatic', 'weapons', 'bowman', 'is', 'with', 'armenian', 'settled', 'new', 'aid', 'john', 'included', 'vector', 'tape', 'gladly', 'hut', 'by', 'member', 'province', '500kbit', 'complex', 'examine', 'point', 'journalist', 'able', 'metzger', 'memphis', 'killings', 'disarmed', 'confused', 'context', 'publish', 'him', 'believe', '1939', 'clever', 'grey', 'who', 'heliocentric', 'supportive', '1', 'seeked', 's', 'then', 'expected', 'pomposity', 'follow', 'disappointed', 'june', 'ankara', 'convention', 'marx', 'different', 'cried', 'discussing', 'real', 'dirty', 'happily', 'refresh', 'calls', 'concert', 'such', 'mizraim', 'ed', 'an', 'actually', 'after', 'thelema', 'divinity', 'adhere', 'women', 'doing', 'courage', 'depending', 'compatability', 'example', 'social', 'think', 'initiations', 'haven', 'involved', 'its', 'notion', 'properties', 'envisioned', 'performance', 'ready', 'experienced', 'quite', 'reason', 'order', 'historical', 'relay', 'means', 'defense', 'rates', 'boasts', 'empire', 'karl', 'designed', 'reduce', 'wynn', 'parts', 'day', 'when', 'papers', 'wanting', 'stretch', 'congress', 'select', 'geniuses', 'kerb', 'us', 'contains', 'those', 'taught', 'my', '2', 'revise', 'kingdom', 'husband', 'praise', 'states', 'hope', 'all', 'been', 'btw', 'democracy', '8', 'steiner', 'basic', '1776', 'regional', 'u', 'vision', 'x', 'having', 'm', 'courses', 'argument', 'tasking', 'purchased', 'pure', 'imaging', 'major', 'stand', 'worse', 'used', 'leading', 'laundry', 'safety', 'various', 'imo', 'suggestions', 'down', 'do', 'driver', 'standing', 'troops', 'know', 'room', 'empted', 'comment', 'interesting', 'reasons', 'present', 'tastes', 'groups', 'response', 'important', 'jersey', 'glass', 'untrustworthy', 'few', 'season', 'accused', 'session', 'info', 'personal', 'area', 'rejigged', 'sanctissimus', '7', 'rightfully', 'criticized', '1888', 'about', 'introduction', 'enforce', 'wasn', 'artists', 'got', 'numbers', 'difference', 'body', 'that', 'padover', 'just', 'much', 'memory', 'publication', 'strong', 'animals', 'speed', 'take', 'matter', 'killer', '6060', 'successfully', 'section', 'quid', 'berlin', 'dream', 'government', '1922', 'book', 'law_', 'he', '1905', 'now', 'utility', 'beta', 'any', 'heavily', 'constitute', 'first', 'theodore', 'contact', 'indeed', 'head', 'set', 'over', 'urge', 'since', 'another', 'wow', 'to', 'following', 'audience', 'workers', 'william', 'rec', 'shoot', 'old', 'c', 'february', 'ix', 'beat', '1895', 'la', 'shows', 'boyd', 'b', 'guidelines', 'into', 'steady', 'series', '1855', 'decreasing', 'has', 'end', 'refering', 'announced', 'paper', 'eyes', 'frees', 'lower', 'diamond', 'say', 'market', 'email', 'house', 'kirlian', 'spy', 'use', 'resistance', 'very', 'knew', 'longstanding', 'committee', 'carry', 'victim', 'build', 'mason', 'manner', 'entered', 'word', 'wall', 'fun', 'p', '1850', 'visual', 'o', 'furthermore', 'using', 'back', 'longer', 'vesa', 'veritatem', 'pittsburghers', 'understand', 'lore', 'two', 'ability', 'cover', 'casserole', 'break', 'laid', 'occurred', 'above', 'perspective', 'hypothesize', 'best', 'encompassed', 'stars', 'met', 'j', 'killing', 'ram', 'recording', 'ultra', 'processes', 'germany', 'unfolded', 'supplied', 'samuel', '20', 'both', 'information', 'detach', 'born', 'austrian', 'bear', 'resolve', 'unjust', 'lets', 'experiments', 'bell', 'serious', 'bands', 'broke', 'can', 'attempts', 'send', 'ad', 'exercising', 'resigned', 'whole', 'practical', 'whoever', 'however', 'multi', 'influence', '10', 'normally', 'outer', 'arms', 'pulp', 'ride', 'against', 'azerbadjan', 'far', 'low', 'fooling', 'goes', 'bit', 'did', 'ounce', 'armenians', 'examiner', 'swath', 'upsate', 'tv', 'wants', 'preserve', 'skeptics', 'we', 'concept', 'found', 'police', 'amount', 'circle', 'july', 'closely', 'deeply', 'thrown', 'continuity', 'repealed', 'platen', 'gyroscopes', 'typically', 'feasts', 'geneva', 'safe', 'sp', 'initiated', 'between', 'raised', 'sabre', 'enough', '00', 'organization', 'counted', 'virginia', 'seeking', 'turned', 'revealing', 'usual', 'plastic', 'regular', 'afternoon', '_an', 'if', '200th', 'her', 'suleyman', 'industrialist', 'united', 'efforts', 'labs', 'are', 'illustration', 'fascist', 'intuition', 'become', 'westcott', 'planes', '30', 'go', 'opinion', 'bashers', 'it', 'high', 'character', 'seems', 'differently', 've', 'hockey', 'insist', 'pass', 'little', 'shearson', 'most', 'technique', 'miller', 'masonry', 'paragraph', 'shotgun', 'around', 'christian', 'fear', 'search', 'jagr', 'rites', 'greater', 'ap', 'merlinus', 'cryptography', 'flew', 'believes', 'klein', 'started', 'became', 'widely', 'wrote', 'giving', 'letter', 'commentaters', 'lack', 'spirit', 'see', 'stands', 'tsn', 'should', 'bay', 'them', 'right', 'growth', 'trying', 'get', 'peoples', 'licence', 'ohhhh', '1925', 'tech', 'some', 'deleted', 'instructed', 'king', 'known', 'ozal', 'watching', 'up', 'transfer', 'explosive', 'feature', 'killed', 'leads', 'initiation', 'magic', 'augsburg', 'associated', '32', '1902', 'restriction', 'power', 'thou', 'facist', '15', 'eyebrows', 'mediterranean', 'reuss', 'policy', 'infiltrating', 'together', 'but', 'sure', 'may', 'because', 'sweden', '3', 'picture', 'rudolph', 'thomas', 'afresh', 'video', 'subcommittee', 'short', 'fans', 'aura', 'want', 'ohhh', 'mass', 'bread', 'thus', 'kind', '1993', 'cultures', 'consecutive', 'warned', 'stretching', 'commercial', 'puzzled', 'before', 'modify', 'containing', 'connect', 'itself', 'employed', 'attempt', 'busy', 'wilt', '2mb', 'looking', 'anyone', 'state', 'larger', 'simplistic', 'try', 'doctrines', 'ways', 'summus', 'accepted', 'instead', 'maybe', 'someone', 'techmar', '1990', 'probably', 'throughly', 'interpretations', 'ideas', 'called', 'leftover', 'brother', 'welcome', 'dallas', 'each', 'health', 'calibre', '241', 'nights', 'accident', 'patriots', '_turkey', 'interpretation', 'way', 'had', 'inescapable', 'every', 'gotten', 'strongest', 'inexpensive', 'difficult', 'things', 'azeri', 'country', 'fifty', 'possession', 'paint', 'on', 'meeting', 'choice', 'dthe', 'rule', 'world', 'turkish', 'history', 'mdp', 'abandoned', 'computers', 'system', 'supports', 'code', 'fervor', 'play', 'based', 'what', 'myself', 'children', 'his', 'frater', 'vapor', 'service', 'experimental', 'from', 'more', 'papus', 'devices', 'devils', 'content', 'a', 'original', 'europe', 'calendar', 'jubilee', 'also', 'aeon', 'several', 'balkans', 'stated', 'let', 'freezes', 'stop', 'requires', '6', 'hopefully', 'buit', 'death', 'flaming', 'pro', 'sic', 'conditions', 'have', 'doesn', 'federal', 'webster', 'common', 'i', 'locking', 'socialist', 'ancient', 'formatters', 'and', 'local', 'bruin', 'include', 'orientis', 'so', 'pizza', 'remain', 'yet', 'islanders', 'revelation', 'tyranny', 'personel', 'detailed', 'drive', 'principle', 'raped', 'freedom', 'through', 'ford', '930418', 'kent', 'behind', 'qualified', 'kurds', 'dma', 'pictures', '250kbit', 'chemist', 'applied', 'was', 'telecasts', 'said', 'away', 'etc', 'the', 'derived', 'magical', 'imho', 'helping', 'whose', 'knowledge', 'move', 'tried', 'farenheit', 'themselves', 'moral', 'type', 'own', 'leaves', 'wolf', 'confines', 'lamb', 'established', 'within', 'critical', 'invoked', 'while', 'franz', 'senate', 'post', 'thank', 'kellner', 'promulgating', 'rex', '150', 'she', 'truelove', 'torture', 'indicates', 'sin', '4', 'jefferson', '_', 'faltering', 'turanist', 'politics', 'com', 'nice', '9', 'weiser', 'wish', 'avoid', 'full', 'yarker', 'couple', 'feel', 'blood', 'isn', 'remember', 'days', 'hymenaeus', 'across', 'will', 'odd', 'proclaimed', 'why', 'equinox', 'golden', 'adepts', 'sec', 'current', 'wings', 'school', 'buff', 'lot', 'beckup', 'psychologists', 'quote', 'like', 'replied', 'decision', 'commands', 'basically', 'hucksters', 'lab', 'possibly', 'data', 'other', 'your', 'explanations', 'sort', 'non', 'stealth', 'in', 'massacre', 'disk', 'man', 'perry', 'matt', 'tapes', 'sawed', 'attack', 'majestic', 'ati', 'mon', '1280', 'as', '120kvolt', 'work', 'armenia'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    " \n",
    "# --- Tokenizer Function ---\n",
    "def tokenize(text):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return tokens\n",
    " \n",
    "# --- Parameters ---\n",
    "num_files = 20  # Number of files to process\n",
    " \n",
    "# --- Process Files ---\n",
    "print(f\"\\n--- TOKENIZING FIRST {num_files} FILES FROM '{input_dir}' ---\\n\")\n",
    " \n",
    "# List and sort .txt files in the directory\n",
    "all_txt_files = sorted([f for f in os.listdir(input_dir) if f.endswith(\".txt\")])\n",
    "all_tokens = set()\n",
    "# Limit to the first `num_files`\n",
    "for i, filename in enumerate(all_txt_files[:num_files], start=1):\n",
    "    filepath = os.path.join(input_dir, filename)\n",
    "    # Read file content\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    " \n",
    "    # Tokenize\n",
    "    tokens = tokenize(text)\n",
    "    # Add unique tokens to the set\n",
    "    all_tokens.update(tokens)\n",
    "\n",
    "print(all_tokens)  # Preview first 20 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed76ec",
   "metadata": {},
   "source": [
    "## 🔁 Step 3: Normalization Pipeline (Stemming, Stop Word Removal, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f277a0d",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> Now we normalize tokens: convert to lowercase, remove stop words, apply stemming or affix stripping. This reduces redundancy and enhances search accuracy.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Use `nltk` to remove stopwords and apply stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66ae9a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['relat', 'activ', 'develop', '5', 'eventu', 'good', 'quot', 'show', 'edit', 'eye', 'next', 'line', 'demirel', 'understand', 'shall', 'irrespect', '97th', 'pretens', 'someth', 'assist']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hitha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def normalize_tokens(tokens):\n",
    "    return [stemmer.stem(t) for t in tokens if t not in stop_words]\n",
    "\n",
    "# Example: normalize one document\n",
    "norm_tokens = normalize_tokens(all_tokens)\n",
    "print(norm_tokens[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34cf58",
   "metadata": {},
   "source": [
    "## 🔍 Step 4: Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c39dd",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> We now map each normalized token to the list of document IDs in which it appears. This is the core structure that allows fast Boolean and phrase queries.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Build the inverted index using a dictionary.\n",
    "- Add code to support phrase queries using positional indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ca8f106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sure': [1, 5, 17], 'basher': [1], 'pen': [1], 'fan': [1], 'pretti': [1], 'confus': [1, 14], 'lack': [1], 'kind': [1, 11], 'post': [1, 3, 5, 12], 'recent': [1]}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_inverted_index(documents):\n",
    "    index = defaultdict(list)\n",
    "    for doc_id, text in enumerate(documents):\n",
    "        tokens = normalize_tokens(tokenize(text))\n",
    "        seen = set()\n",
    "        for token in tokens:\n",
    "            if token not in seen:\n",
    "                index[token].append(doc_id + 1)\n",
    "                seen.add(token)\n",
    "    return index\n",
    "\n",
    "inverted_index = build_inverted_index(documents)\n",
    "print(dict(list(inverted_index.items())[:10]))  # Preview first 10 terms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef4df8",
   "metadata": {},
   "source": [
    "## 🧪 Test: Phrase Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db832216",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> A phrase query requires the exact sequence of terms (e.g., \"machine learning\"). To support this, extend the inverted index to store positions, not just docIDs.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Implement 2 phrase queries.\n",
    "- Demonstrate that they return the correct documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97132fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for phrase query implementation\n",
    "# You may build a position-aware index or use string search within docs after normalization\n",
    "def phrase_query(inverted_index, phrase, stemmer):\n",
    "    tokens = [stemmer.stem(w) for w in phrase.strip().split()]\n",
    "    if not tokens:\n",
    "        return set()\n",
    "    \n",
    "    # Get doc sets for each token\n",
    "    doc_sets = []\n",
    "    for token in tokens:\n",
    "        if token not in inverted_index:\n",
    "            return set()\n",
    "        doc_sets.append(set(inverted_index[token]))\n",
    "    \n",
    "    # Return intersection\n",
    "    return set.intersection(*doc_sets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3a7897d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase found in: {1, 5, 17}\n",
      "Phrase found in: {16, 6, 15}\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "query1 = \"sure\"\n",
    "docs = phrase_query(inverted_index, query1, stemmer)\n",
    "print(\"Phrase found in:\", docs)\n",
    "\n",
    "query2 = \"devices\"\n",
    "docs = phrase_query(inverted_index, query2, stemmer)\n",
    "print(\"Phrase found in:\", docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
