{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890288b1",
   "metadata": {},
   "source": [
    "# 🛠️ Active Learning Workshop: Implementing an Inverted Matrix (Jupyter + GitHub Edition)\n",
    "## 🔍 Workshop Theme\n",
    "*Readable, correct, and collaboratively reviewed code—just like in the real world.*\n",
    "### Team:\n",
    "- **Zhimin Xiong** \n",
    "- **Yu-Chen Chou**\n",
    "- **Haysam Elamin**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e78d18",
   "metadata": {},
   "source": [
    "\n",
    "Welcome to the 90-minute workshop! In this hands-on session, your team will build an **Inverted Index** pipeline, the foundation of many intelligent systems that need fast and relevant access to text data — such as AI agents.\n",
    "\n",
    "### 👥 Team Guidelines\n",
    "- Work in teams of 3.\n",
    "- Submit one completed Jupyter Notebook per team.\n",
    "- The final notebook must contain **Markdown explanations** and **Python code**.\n",
    "- Push your notebook to GitHub and share the `.git` link before class ends.\n",
    "\n",
    "---\n",
    "## 🔧 Workshop Tasks Overview\n",
    "\n",
    "1. **Document Collection**\n",
    "2. **Tokenizer Implementation**\n",
    "3. **Normalization Pipeline (Stemming, Stop Words, etc.)**\n",
    "4. **Build and Query the Inverted Index**\n",
    "\n",
    "> Each step includes a sample **talking point**. Your team must add your own custom **Markdown + code cells** with a **second talking point**, and test your Inverted Index with **2 phrase queries**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a922333",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 🧠 Learning Objectives\n",
    "- Implement an **Inverted Matrix** using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into peer notebooks.\n",
    "- Practice **collaborative debugging**, professional peer feedback, and improve code quality.\n",
    "\n",
    "## 🧩 Workshop Structure (90 Minutes)\n",
    "1. **Instructor Use Case Introduction** *(15 min)* – Set up teams of 3 people. Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Team Jupyter Notebook Development** *(45 min)* – Manual IR and Inverted Matrix coding + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** *(15 min)* – Teams commit and push initial notebooks. **Make sure to include your names so it is easy to identify the team that developed the Min-Max code**.\n",
    "4. **Instructor Review** - The instructor will go around, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "5. **Email Delivery** *(15 min)* – Each team send the instructor an email **with the *.git link** to the GitHub repo **(one email/team)**. Subject on the email is: PROG8245 - Inverted Matrix  Workshop, Team #_____.\n",
    "\n",
    "\n",
    "## 💻 Submission Checklist\n",
    "- ✅ `IR_InvertedMatrix_Workshop.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline, and Inverted Index.\n",
    "  - Markdown explanations for each major step\n",
    "  - **Labeled talking point(s)** and 2 phrase query tests\n",
    "- ✅ `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- ✅ GitHub Repo:\n",
    "  - Public repo named `IR-invertedmatrix-workshop`\n",
    "  - This is a group effort, so **choose one member of the team** to publish the repo\n",
    "  - At least **one commit containing one meaningful talking point**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e160c9d",
   "metadata": {},
   "source": [
    "## 📄 Step 1: Document Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc964464",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> We begin by gathering a text corpus. To build a robust index, your vocabulary should include **over 2000 unique words**. You can use scraped articles, academic papers, or open datasets.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Collect at least 20+ text documents.\n",
    "- Ensure the vocabulary exceeds 2000 unique words.\n",
    "- Load the documents into a list for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23ee0c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 documents.\n"
     ]
    }
   ],
   "source": [
    "# Example: Load text files from a folder\n",
    "import os\n",
    "import glob\n",
    "\n",
    "input_dir = 'sample_docs/'\n",
    "\n",
    "def sorted_doc_filenames(path=\".\", prefix=\"doc\", suffix=\".txt\"):\n",
    "    # Get all matching files like doc1.txt, doc2.txt, ...\n",
    "    files = glob.glob(f\"{path}/{prefix}*[0-9]{suffix}\")\n",
    "    \n",
    "    # Sort numerically based on number in filename\n",
    "    files.sort(key=lambda x: int(re.search(rf\"{prefix}(\\d+){suffix}\", x).group(1)))\n",
    "    \n",
    "    return files\n",
    "\n",
    "def load_documents(file_paths):\n",
    "    documents = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            documents.append(f.read())\n",
    "    return documents\n",
    "\n",
    "# Replace 'sample_docs/' with your actual folder\n",
    "file_paths = sorted_doc_filenames(path=input_dir, prefix=\"doc\", suffix=\".txt\")\n",
    "documents = load_documents(file_paths)\n",
    "print(f\"Loaded {len(documents)} documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b342945a",
   "metadata": {},
   "source": [
    "## ✂️ Step 2: Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2803fb52",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> The tokenizer breaks raw text into a stream of words (tokens). This is the foundation for every later step in IR and NLP.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Implement a basic tokenizer that splits text into lowercase words.\n",
    "- Handle punctuation removal and basic non-alphanumeric filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7c3654e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TOKENIZING FIRST 20 FILES FROM 'sample_docs/' ---\n",
      "\n",
      "{'had', 'original', 'maybe', 'envisioned', 'upon', 'spy', 'citizen', '10', 'explosive', 'empted', 'buff', 'got', 'samuel', 'data', 'armed', 'world', 'laws', 'swedish', 'interpret', 'kellner', 'magical', '1855', 'opinion', 'heavily', '20ms', 'h', 'spoken', 'truelove', 'july', 'bowman', 'leaves', 'background', 'current', 'universal', 'turned', 'before', 'armenians', 'watch', 'served', 'appear', 'kerb', 'ride', 'secret', 'game', 'across', 'appeal', 'regular', 'takes', 'move', 'amount', 'watching', 'hypothesize', 'detach', 'lets', 'divinity', 'suleyman', 'mass', 'development', 'history', '_the', 'appointed', 'body', 'greater', 'gyroscopes', 'azeris', 'constitution', 'worse', 'tech', 'better', 'spirit', 'eyes', 'victim', 'basic', 'code', 'gnostic', '120kvolt', 'writing', 'beware', 'continue', '27', 'properties', 'artists', 'ways', 'amendment', 'around', 'assistant', 'tape', 'that', 'platen', 'soft', 'presented', '1925', 'kind', 'questioned', 'quote', 'shotgun', '30', 'revealing', '17', 'fervor', 'i', 'purchased', 'settled', 'critical', 'allow', '_equinox', 'me', 'than', 'ati', 'thank', 'resolve', 'fair', 'kingdom', 'orientis', '8', 'facist', 'often', 'rejigged', 'commands', 'extremes', 'dream', 'finally', 'moral', 'until', 'tyrants', 'more', 'is', 'vesa', 'magic', 'essentially', 'yarker', 'become', 'law_', 'memphis', 'buit', 'politics', 'enforce', 'held', 'hope', 'bait', 'transfers', 'thelema', 'canada', 'final', 'pass', 'sort', 'masonic', 'processes', 'like', '_turkey', 'drive', 'technique', 'paper', 'clever', 'vouch', 'followed', 'perspectives', 'fo', 'perspective', 'cation', 'ad', 'contact', 'anyone', 'down', 'value', 'matt', 'rate', 'heinrich', 'these', 'holocaust', 'notion', '28', '250kbit', 'kirlian', 'april', 'torture', 'oto', 'common', 'please', 'became', 'vision', 'caliph', 'met', 'permitted', '241', 'having', 'azeri', 'device', 'stands', 'find', 'christian', 'france', 'derived', 'video', 'guns', 'art', 'work', 'giving', 'u', 'actually', 'longer', 'stars', 'gain', 'devices', 'superior', 'started', 'austrian', 'successor', 'likely', '32', 'means', '1895', 'containing', 'about', 'protects', 'individuals', 'mechanism', 'we', 'leading', 'vis', 'employed', 'franz', 'talk', 'social', 'mag', 'surface', 'demirel', 'consecutive', 'lack', 'recent', 'refering', 'wall', 'ever', 'counted', 'ap', 'shall', 'carry', 'once', 'local', 'sp', 'eventually', 'authority', 'side', 'tapes', 'come', 'liberty', 'daughter', 'permit', 'email', 'blood', 'excellent', 'labs', 'lab', 'virginia', 'reasonably', 'arcane', '40', 'ohhh', 'multi', 'john', 'principles', 'idea', 'hartmann', 'isn', 'eruption', 'dawn', '600rpm', 'wish', 'concert', 'together', 'cheers', 'state', 'important', 'case', 'tyranny', 'convention', '1950', 'also', 'infusing', 'cal', 'pulp', 'make', 'merlinus', 'insist', 'itself', 'ends', 'court', 'system', 'bmf', 'own', 'ability', 'psychologists', 'has', 'ultra', 'groups', 'for', 'very', 'after', 'pure', 'longstanding', 'talking', 'rule', 'constitute', 'puzzled', 'several', 'certainly', 'calibre', 'bashers', 'accused', 'upsate', 'personal', 'interpretations', 'rudolph', 'mentioned', 'rear', 'aeon', 'real', '2mb', 'from', 'fuhr', 'leads', 'love', 'killings', 'haven', 'karabag', '300', 'males', 'skeptics', 'detailed', 'bus', 'jefferson', 'back', 'upwards', 'turkey', 'dma', 'cultures', 'market', 'allowed', 'suggest', 'preamble', 'line', 'bought', 'modern', 'sure', 'argument', 'normally', 'larger', 'butter', 'premier', 'bread', 'task', 'must', 'protected', 'them', 'plastic', '15', 'lore', 'quid', 'expected', 'law', 'aquire', 'such', 'millisecond', 'historical', 'sawed', 'commentator', 'stealth', 'guess', 'yet', 'people', 'others', 'each', 'the', 'surely', 'papus', 'century', 'beat', 'extend', 'women', 'max', 'scsi', 'not', 'ago', 'wd40', 'last', 'difficult', 'attacks', 'pretense', 'here', 'p', 'february', 'countersteering', 'fun', 'cover', 'wants', 'limited', 'translated', 'meeting', 'mdp', 'furthermore', 'understand', 'wrote', 'possibly', 'contacts', 'graphics', 'give', 'no', 'peoples', 'unfortunately', 'raped', '2', 'first', 'tortured', '1939', 'motor', '1850', 'techmar', 'lower', 'feel', 'states', 'thanks', 'gladly', 'etc', 'called', 'order', 'workers', 'show', 'lot', 'mediterranean', 'so', 'impartial', 'supply', 'rpm', 'protect', 'yeah', 'hucksters', 'doesn', 'education', 'majestic', 'later', 'few', 'years', 'introduced', 'revelation', 'fox', 'stop', 'phrase', 'ancient', 'play', '4', 'just', 'preservation', 'usual', 'difference', 'rather', 'report', 'my', 'interpreted', 'faq', 'judiciary', 'memory', 'argue', 'best', 'afternoon', 'kinship', 'flaming', 'split', 'intelligent', 'welcome', 'stroke', 'eyebrows', 'computer', 'were', 'frater', 'commentaters', 'perhaps', 'rev', 'patriots', 'in', 'brash', 'abandoned', 'publication', 'major', 'seen', 'regional', 'feet', 'death', 'replied', 'adhere', 'mighty', 'section', 'gotten', 'indeed', 'ed', 'post', 'program', 'examiner', 'manner', 'sweden', 'steady', 'request', 'what', 'health', 'cried', 'strongest', 'seems', 'promulgating', 'milliseconds', 'planes', 'aura', 'it', 'c', 'behavoir', 'paint', 'probably', 'adriatic', 'have', 'official', 'lock', 'context', 'urge', 'resistance', 'supreme', 'perpetually', '_', 'four', 'choice', 'prior', 'one', 'being', 'schooled', 'killed', 'truth', 'publish', 'designed', 'suggestions', 'reduce', 'unfolded', 'place', 'many', 'spectacular', 'two', 'squirted', 'declares', 'able', 'feature', 'something', 'uriquidez', 'congress', 'killer', 'introduction', 'brother', 'german', 'appears', 'perry', 'are', 'militias', 'criticized', 'her', 'concretize', 'hearded', 'suffering', 'get', '930418', 'established', 'shearson', 'organization', 'attempt', '0055', 'noise', 'answers', 'inescapable', 'next', 'joining', 'knowledge', 'can', 'relief', 'set', 'break', 'great', 'sanctissimus', 'thrown', 'assess', 'casserole', 'tsn', '0', 'children', 'discussing', 'dthe', 'shoot', 'been', 'express', 'announced', 'sometimes', 'summus', 'plane', 'historic', 'write', 'imho', 'therefore', '6060', 'tastes', 'related', 'wealthy', 'traveled', 'military', 'personel', 'senate', 'which', 'east', 'showed', 'doctrines', 'armenia', 'ide', 'nj', 'thomas', 'other', 'journalist', 'whose', 'avoid', 'armenian', 'ordained', 'threat', 'his', 'accident', 'build', '1', 'rider', 'conditions', 'shoulder', 'masonry', 'fact', 'faltering', 'could', 'speed', 'paragraph', 'don', 'safe', 'right', 'utility', 'crap', 'telecasts', 'ok', '3', 'modify', 'rex', 'rates', 'most', 'examination', 'swath', 'model', '200th', 'low', 'weight', 'pens', 'ready', '10_', 'surprises', '1982', 'instead', 'cannot', 'weiser', 'term', 'short', 'rites', 'strongly', 's', 'rituals', 'lamb', 'go', 'decision', 'geniuses', 'marx', 'said', 'anyway', 'intuition', 'dribble', 'manyk', 'warned', '9', 'closely', 'service', 'rationale', 'outer', '1993', 'province', 'high', 'speaking', 'its', 'europe', 'search', 'tlu', 'pretend', 'confused', 'believes', 'throughly', 'room', 'info', 'tried', 'principle', 'if', 'united', 'boasts', 'time', 'golden', 'ones', 'grade', 'simplistic', 'theodore', 'hut', 'bruin', 'diamond', 'papers', 'nice', 'good', 'infiltrating', 'proclaimed', 'industrialist', 'farenheit', 'klein', 'government', 'padover', 'behind', 'further', 'different', 'given', 'same', 'w', 'myself', 'chartered', 'occurred', 'cbc', 'encompassed', 'thou', 'of', 'did', 'testimony', 'ordo', 'knew', 'participating', 'v', 'unjust', '600', 'pmetzger', 'question', 'kurds', 'vlb', 'accepted', 'beyond', 'augsburg', 'riding', 'testing', 'thread', 'cold', 'ae', 'efforts', 'practical', 'praise', 'deeply', 'session', 'connect', 'he', 'looked', 'using', '1787', 'modernism', 'day', 'worked', 'afresh', 'singed', 'azerbadjan', 'transfer', 'lewis', 'change', 'christmas', 'response', 'comprised', 'beckup', 'continuity', 'metzger', 'succeeded', 'specific', 'stretch', 'occur', '1888', 'karl', 'whole', 'figures', 'earlier', 'into', 'experienced', 'experimental', 'private', 'pitt', 'try', 'front', 'pre', 'grey', 'information', 'crowley', 'policy', 'disagree', 'fill', 'jvc', 'says', 'growth', 'sea', 'wanting', 'refresh', 'husband', 'iii', 'wolf', 'btw', 'hmm', 'significance', 'fifty', 'over', 'massacre', 'june', 'secrets', 'bruins', 'regarding', 'came', 'acting', 'far', 'weapons', 'licence', 'associated', 'use', 'vapor', 'pan', 'pamphlets', 'germany', 'irrespective', 'berlin', 'thought', 'laughable', 'taken', 'steiner', 'mon', '500kbit', 'stretching', 'orders', 'member', '_an', 'point', 'mizraim', 'veritatem', 'kid', 'militia', 'hopefully', 'every', '1776', 'little', 'moment', 'send', 'remain', 'under', 'frees', 'role_', 'possess', 'interesting', 'gone', 'games', 'days', 'exercising', 'orchid', 'house', 'away', 'at', 'bearing', 'os', 'series', 'attached', 'and', 'recording', 'abc', 'still', 'inexpensive', 'to', 'three', 'ideas', 'wording', 'spencer', 'be', 'changed', 'basically', 'cryptography', 'disappointed', 'jesus', 'explicitly', 'let', 'sic', 'edited', 'book', 'pretty', 'too', 'argumentation', '1912', 'card', 'relieved', 'us', 'rulers', '150', 'there', 'conclusion', 'mason', 'include', 'season', 'heliocentric', 'com', 'locking', 'ix', 'want', 'smith', 'problem', 'price', 'sabre', 'courage', 'calendar', 'taught', 'decreasing', 'jasmine', 'fascist', 'safety', 'didn', 'woefully', 'nights', 'proposal', 'performance', 'audience', 'encompass', 'army', 'imaging', 'out', 'an', 'bit', 'stand', 'themselves', 'couple', '00', 'sanctity', 'their', 'rec', 'illustration', 'formatters', 'pictures', 'anything', 'posts', 'feeble', 'holy', 'or', 'might', 'starting', 'dallas', 'wow', 'various', 'man', 'honestly', 'liberties', 'goes', 'word', 'sexual', '1280', 'explain', 'thus', 'disks', 'sec', 'refreshed', 'visual', 'sf', 'stuff', 'restriction', 'displays', '6', 'much', 'political', 'ram', 'either', 'look', 'found', 'founded', 'bad', 'beta', 'balkans', 'things', 'sci', 'teeth', 'webster', 'laid', 'content', 'vector', 'words', 'members', 'attack', 'versed', 'requesting', 'half', 'born', '1876', 'doing', 'forth', 'centralization', 'doubts', 'qualified', 'typically', 'strong', 'start', 'apprehend', 'untrustworthy', 'mean', 'numbers', 'fear', 'then', 'take', 'free', 'imo', 'instructed', 'may', 'ask', 'matter', 'committee', 'will', 'lies', 'equinox', 'dirty', 'authorization', 'because', 'turkic', 'men', 'who', '20', 'direct', 'justified', 'above', 'ford', 'does', 'pay', 'yesterday', 'influence', 'carefully', 'preserve', 'now', 'william', 'up', 'already', 'person', 'redcross', 'boyd', 'integrity', 'police', 'gic', 'perception', 'involved', 'true', 'troops', 'english', 'interjection', 'bands', 'example', 'your', 'pomposity', 'almost', 'circle', 'resort', 'although', 'possession', '7', 'since', 'defense', 'practices', 'bay', 'fight', 'russian', 'skills', 'stated', 'deleted', 'kent', 'hockey', 'those', 'based', 'bell', 'flew', 'countersteer', 'initiated', 'concept', 'components', 'less', 'imagine', 'following', 'turanist', 'ozal', 'all', 'confines', 'prosecution', 'widely', 'represent', 'aid', 'laundry', 'league', 'jersey', 'moments', 'she', 'put', 'playoffs', 'bet', 'type', 'x', 'rightfully', 'possibilities', 'whoever', 'teletype', 'commercial', '97th', 'guidelines', 'prussian', 'someone', 'revise', 'reuss', 'looking', 'j', 'discuss', 'though', 'tasking', 'noah', 'against', 'known', 'am', 'age', 'federal', 'chemist', 'but', 'bos', 'pro', 'bbc', 'hard', 'compatability', 'on', 'leadership', '5', 'animals', 'transfered', 'pittsburghers', 'quoting', 'know', 'corner', 'successfully', 'galacticentric', 'publishing', 'belong', 'applied', 'named', 'head', 'oriflamme', 'select', 'none', 'debarred', '214', 'templi', 'both', 'resigned', 'explanations', 'turkish', 'b', 'depending', 'survival', 'initiation', 'fooling', 'how', 'democracy', 'wynn', 'killing', 'when', 'broke', 'reason', 'leader', 'comment', 'shows', 'country', 'wings', 'included', 'freezes', 'seemed', 'examine', 'caucasus', 'contains', 'shackled', 'requires', 'some', 'with', 'magick', 'impressed', 'm', 'seeking', 'busy', 'however', 'entered', 'helping', 'turks', 'odd', 'including', 'going', 'feasts', 'through', 'solution', 'area', 'our', '1990', 'nobody', 'lie', 'wishing', 'skim', 'lose', 'expression', '1905', 'sin', 'arise', 'invoked', 'limits', 'happily', 'driver', 'sacrifices', 'raised', 'differently', 'supports', 'leftover', 'trying', 'this', 'king', 'issue', 'wilt', 'parts', 'disarmed', 'letter', 'putting', 'l', 'jubilee', 'by', 'fans', 'published', 'devils', 'que', 'glass', 'bear', 'between', 'relay', 'sword', 'wasn', 'repealed', 'material', 'self', 'disk', 'socialist', 'quite', 'standing', 'see', 'usa', 'while', 'adult', 'steve', 'was', 'seeked', 'possible', 'requirement', 'easy', 'non', 'enough', 'serious', 'singer', 'bunch', 'hymenaeus', 'way', 'follow', 'power', 'you', 'retain', 'interpretation', 'dogmatic', 'jagr', 'empire', 'indicates', 'supplied', 'end', 'subcommittee', 'forgot', 'miller', 'new', 'islanders', 'keep', 'unusual', 'freedom', 'another', 'china', '334', 'nothing', 'actively', 'ratifi', 'computers', 'think', 'geneva', 'they', 'character', 'radius', 'experimenters', 'any', 'individual', 'invoke', 'capable', '1902', 'present', 'ankara', 'second', 'should', 'experiments', 'america', 'o', 'remember', 'thousands', 'calls', 'pizza', 'picture', 'off', 'tree', 'physically', 'a', 'test', 'firearms', 'peaceful', 'him', 'ohhhh', 'force', 'courses', 'why', 'where', 'rapings', 'initiations', 'edition', 'brake', 'eye', 'understanding', 'arms', 'support', 'ounce', 'well', 'supportive', 'westcott', 'job', 'assisted', 'stats', 'school', 'do', 'within', 'adepts', 'heart', 'believe', 'tv', 'made', 'even', 'la', 'say', 'only', 'used', 've', 't', 'reasons', 'uerdugo', 'full', 'develop', '1922', 'complex', 'as', 'attempts', 'defines', 'would', 'old'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    " \n",
    "# --- Tokenizer Function ---\n",
    "def tokenize(text):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return tokens\n",
    " \n",
    "# --- Parameters ---\n",
    "num_files = 20  # Number of files to process\n",
    " \n",
    "# --- Process Files ---\n",
    "print(f\"\\n--- TOKENIZING FIRST {num_files} FILES FROM '{input_dir}' ---\\n\")\n",
    " \n",
    "# List and sort .txt files in the directory\n",
    "all_txt_files = sorted([f for f in os.listdir(input_dir) if f.endswith(\".txt\")])\n",
    "all_tokens = set()\n",
    "# Limit to the first `num_files`\n",
    "for i, filename in enumerate(all_txt_files[:num_files], start=1):\n",
    "    filepath = os.path.join(input_dir, filename)\n",
    "    # Read file content\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    " \n",
    "    # Tokenize\n",
    "    tokens = tokenize(text)\n",
    "    # Add unique tokens to the set\n",
    "    all_tokens.update(tokens)\n",
    "\n",
    "print(all_tokens)  # Preview first 20 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed76ec",
   "metadata": {},
   "source": [
    "## 🔁 Step 3: Normalization Pipeline (Stemming, Stop Word Removal, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f277a0d",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> Now we normalize tokens: convert to lowercase, remove stop words, apply stemming or affix stripping. This reduces redundancy and enhances search accuracy.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Use `nltk` to remove stopwords and apply stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66ae9a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['origin', 'mayb', 'envis', 'upon', 'spi', 'citizen', '10', 'explos', 'empt', 'buff', 'got', 'samuel', 'data', 'arm', 'world', 'law', 'swedish', 'interpret', 'kellner', 'magic']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\xiong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def normalize_tokens(tokens):\n",
    "    return [stemmer.stem(t) for t in tokens if t not in stop_words]\n",
    "\n",
    "# Example: normalize one document\n",
    "norm_tokens = normalize_tokens(all_tokens)\n",
    "print(norm_tokens[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34cf58",
   "metadata": {},
   "source": [
    "## 🔍 Step 4: Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c39dd",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> We now map each normalized token to the list of document IDs in which it appears. This is the core structure that allows fast Boolean and phrase queries.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Build the inverted index using a dictionary.\n",
    "- Add code to support phrase queries using positional indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ca8f106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sure': defaultdict(<class 'list'>, {1: [0], 6: [32], 13: [64]}), 'basher': defaultdict(<class 'list'>, {1: [1]}), 'pen': defaultdict(<class 'list'>, {1: [2, 10, 27, 55, 69]}), 'fan': defaultdict(<class 'list'>, {1: [3]}), 'pretti': defaultdict(<class 'list'>, {1: [4]}), 'confus': defaultdict(<class 'list'>, {1: [5], 3: [69, 97]}), 'lack': defaultdict(<class 'list'>, {1: [6]}), 'kind': defaultdict(<class 'list'>, {1: [7], 19: [181]}), 'post': defaultdict(<class 'list'>, {1: [8], 2: [33], 11: [12], 13: [603]}), 'recent': defaultdict(<class 'list'>, {1: [9]})}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_inverted_index(documents):\n",
    "    index = defaultdict(lambda: defaultdict(list))\n",
    "    for doc_id, text in enumerate(documents):\n",
    "        tokens = normalize_tokens(tokenize(text))\n",
    "        for pos, token in enumerate(tokens):\n",
    "            index[token][doc_id + 1].append(pos)\n",
    "    return index\n",
    "\n",
    "inverted_index = build_inverted_index(documents)\n",
    "print(dict(list(inverted_index.items())[:10]))  # Preview first 10 terms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef4df8",
   "metadata": {},
   "source": [
    "## 🧪 Test: Phrase Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db832216",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> A phrase query requires the exact sequence of terms (e.g., \"machine learning\"). To support this, extend the inverted index to store positions, not just docIDs.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Implement 2 phrase queries.\n",
    "- Demonstrate that they return the correct documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97132fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for phrase query implementation\n",
    "# You may build a position-aware index or use string search within docs after normalization\n",
    "def phrase_query(index, phrase):\n",
    "    phrase_tokens = normalize_tokens(tokenize(phrase))\n",
    "    if not phrase_tokens:\n",
    "        return []\n",
    "\n",
    "    # All tokens must exist in index\n",
    "    if any(token not in index for token in phrase_tokens):\n",
    "        return []\n",
    "\n",
    "    # Get posting lists for each token\n",
    "    posting_lists = [index[token] for token in phrase_tokens]\n",
    "    \n",
    "    # Find common documents containing all tokens\n",
    "    common_docs = set(posting_lists[0].keys())\n",
    "    for postings in posting_lists[1:]:\n",
    "        common_docs &= postings.keys()\n",
    "    \n",
    "    result_docs = []\n",
    "\n",
    "    for doc_id in common_docs:\n",
    "        positions_lists = [postings[doc_id] for postings in posting_lists]\n",
    "\n",
    "        # Check for sequential positions\n",
    "        for start_pos in positions_lists[0]:\n",
    "            if all((start_pos + offset) in positions_lists[offset] for offset in range(1, len(phrase_tokens))):\n",
    "                result_docs.append(doc_id)\n",
    "                break\n",
    "\n",
    "    return result_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3a7897d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase high school found in: [6]\n",
      "Phrase devices found in: [4, 5, 14]\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "query1 = \"high school\"\n",
    "docs = phrase_query(inverted_index, query1)\n",
    "print(f\"Phrase {query1} found in:\", docs)\n",
    "\n",
    "query2 = \"devices\"\n",
    "docs = phrase_query(inverted_index, query2)\n",
    "print(f\"Phrase {query2} found in:\", docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
